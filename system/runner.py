import logging
import time
from pathlib import Path
from typing import TypedDict

import scipy
import torch
from transformers import AutoTokenizer, PreTrainedModel, PreTrainedTokenizer, BertForSequenceClassification, pipeline, \
    Pipeline, AutoModelForSequenceClassification

from .common.config import Interaction, CompilerConfigV2, Reply

# Initialize logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class Prediction(TypedDict):
    """
    Represents a prediction with a label and its associated score.

    :ivar label: The label or category of the prediction.
    :type label: str
    :ivar score: The confidence score or probability of the prediction.
    :type score: float
    """
    label: str
    score: float


class ClassificationResult:
    confidence_margin: float
    sorted_predictions: list[Prediction]

    def __init__(self, predictions: list[Prediction]):
        sorted_predictions = sorted(predictions, key=lambda x: x["score"], reverse=True)

        top_prediction = sorted_predictions[0]
        second_prediction = sorted_predictions[1] if len(sorted_predictions) > 1 else None

        # Calculate the confidence margin
        if second_prediction:
            confidence_margin = top_prediction["score"] - second_prediction["score"]
        else:
            confidence_margin = top_prediction["score"]

        probs = list(map(lambda x: x["score"], sorted_predictions))
        entropy = scipy.stats.entropy(probs, base=2)

        self.sorted_predictions = sorted_predictions
        self.confidence_margin = confidence_margin

    def get_label(self, rank=0):
        return self.sorted_predictions[rank]["label"]

    def get_prediction(self, rank=0):
        return self.sorted_predictions[rank] if rank < len(self.sorted_predictions) else None

    def __repr__(self):
        return (f"ClassificationResult(sorted_predictions={self.sorted_predictions}, "
                f"top_prediction={self.get_prediction()}, "
                f"second_prediction={self.get_prediction(1)}, "
                f"confidence_margin={self.confidence_margin})")

    def __str__(self):
        p = self.get_prediction()
        return (f"Top Prediction: {p['label']} with score {p['score']:.2f}, "
                f"Confidence Margin: {self.confidence_margin:.2f}")


class BertModelComponents:
    """
    Represents a container for a pre-trained model, tokenizer, and classification pipeline.

    This class holds components required for text/classification tasks using
    pre-trained models. It stores the model, tokenizer, and classification pipeline,
    providing functionality for text classification.

    :ivar model: The pre-trained model used for performing the text classification.
    :type model: PreTrainedModel
    :ivar tokenizer: The tokenizer associated with the given pre-trained model.
    :type tokenizer: PreTrainedTokenizer
    :ivar classifier: The classification pipeline that uses the model and tokenizer
        to perform text classification.
    :type classifier: Pipeline
    """

    def __init__(self, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, classifier: Pipeline):
        self.model = model
        self.tokenizer = tokenizer
        self.classifier = classifier

    def classify(self, text: str) -> ClassificationResult:
        """
        Classifies the given text using the initialized model and returns a list of
        predictions. The function uses a pre-trained classifier model to analyze
        the input text and generate predictions based on it. The duration of the
        classification process is logged for debugging purposes.

        :param text: The input text to be classified.
        :type text: str
        :return: A list of predictions generated by the classifier for the input text.
        :rtype: list[Prediction]
        """
        logger.debug(f"Using model: {self.model.config.name_or_path} to classify: \"{text}\"...")

        start_time = time.time()
        # The Pipeline object implements __call__, so this is valid.
        # The object returned is
        predictions: list[Prediction] = self.classifier(text)[0]
        end_time = time.time()

        logger.debug(f"Classification took {end_time - start_time} seconds. It returned: {predictions}")

        return ClassificationResult(predictions)

    @staticmethod
    def from_loaded(model: PreTrainedModel, tokenizer: PreTrainedTokenizer):
        classifier = pipeline(model=model, tokenizer=tokenizer, task="text-classification",
                              topk=None,  # by setting top_k to None we get all the predictions.
                              device=torch.cuda.current_device())

        return BertModelComponents(model, tokenizer, classifier)

    @staticmethod
    def from_path(model_name: str, conf_artifact_path: Path, subdir: str = "trained_model") -> 'BertModelComponents':
        """
        Loads a pre-trained model, tokenizer, and classifier for text classification from the specified
        configuration artifact path and subdirectory. Returns the components wrapped in a ModelComponents
        object. The function handles the initialization of the model and tokenizer using their respective
        libraries, and sets up a text classification pipeline.

        :param model_name: The name of the model which is used to locate its files inside the configuration
            artifact directory.
        :type model_name: str

        :param conf_artifact_path: The base path to the directory containing the configuration artifacts
            for multiple models.
        :type conf_artifact_path: Path

        :param subdir: The subdirectory inside the model directory that contains the trained model data.
            Defaults to "trained_model".
        :type subdir: str

        :return: A ModelComponents object containing the loaded model, tokenizer, and classifier pipeline.
        :rtype: BertModelComponents
        """
        try:
            path = conf_artifact_path / model_name / subdir
            model = BertForSequenceClassification.from_pretrained(path)
            tokenizer = AutoTokenizer.from_pretrained(path)

            return BertModelComponents.from_loaded(model, tokenizer)
        except Exception as er:
            logger.error(f"Error loading model '{model_name}': {er}")
        raise

    def __repr__(self):
        return f"ModelTokenizerPair(model={self.model}, tokenizer={self.tokenizer})"


def load_model(model_name: str, conf_artifact_path: Path, subdir: str = "trained_model") -> BertModelComponents:
    """
    Loads a pre-trained model, tokenizer, and classifier for text classification from the specified
    configuration artifact path and subdirectory. Returns the components wrapped in a ModelComponents
    object. The function handles the initialization of the model and tokenizer using their respective
    libraries, and sets up a text classification pipeline.

    :param model_name: The name of the model which is used to locate its files inside the configuration
        artifact directory.
    :type model_name: str

    :param conf_artifact_path: The base path to the directory containing the configuration artifacts
        for multiple models.
    :type conf_artifact_path: Path

    :param subdir: The subdirectory inside the model directory that contains the trained model data.
        Defaults to "trained_model".
    :type subdir: str

    :return: A ModelComponents object containing the loaded model, tokenizer, and classifier pipeline.
    :rtype: BertModelComponents
    """
    try:
        path = conf_artifact_path / model_name / subdir
        model = AutoModelForSequenceClassification.from_pretrained(path)
        tokenizer = AutoTokenizer.from_pretrained(path)
        classifier = pipeline(model=model, tokenizer=tokenizer, task="text-classification",
                              top_k=None,  # by setting top_k to None we get all the predictions.
                              device=torch.cuda.current_device())
        return BertModelComponents(model, tokenizer, classifier)
    except Exception as er:
        logger.error(f"Error loading model '{model_name}': {er}")
        raise


def load_bert_models(conf_artifact_path: Path, root_interaction: Interaction) -> dict[str, BertModelComponents]:
    """
    Loads all the necessary pre-trained models and their respective components
    like the tokenizer and classifier pipeline for text classification. This
    function identifies the models required based on the root interaction, loads
    each model, and handles any errors encountered during the loading process.
    If any critical error occurs during model loading, it halts the startup
    process.

    :param conf_artifact_path: Path to the configuration artifact directory where
        model directories are located.
    :type conf_artifact_path: Path
    :param root_interaction: An interaction object that helps to determine
        the models/resources that need to be loaded.
    :type root_interaction: Interaction
    :return: A dictionary mapping model names to their corresponding components
        (model, tokenizer, classifier pipeline).
    :rtype: dict[str, BertModelComponents]
    :raises RuntimeError: If some or all models fail to load successfully,
        halting the process with an error message.
    """

    required_models = root_interaction.discover_resources_to_load()
    models: dict[str, BertModelComponents] = {}
    errors = []

    for r in required_models:
        try:
            models[r] = load_model(r, conf_artifact_path)
        except Exception as e:
            errors.append(f"Model '{r}' failed to load: {e}")

    if errors:
        error_message = "\n".join(errors)
        logger.error(f"Failed to load all required models:\n{error_message}")
        raise RuntimeError(f"Startup process halted due to model loading errors:\n{error_message}")

    return models


def run_runner(config_path: Path, artifacts_dir: Path = Path(".")):
    config = CompilerConfigV2.load_from_file(config_path)
    conf_artifact_path = artifacts_dir / config.name

    starting_flow = config.flows.get("main")

    current_flow = starting_flow
    current_step = starting_flow.steps.get(current_flow.start_step)

    context = {}

    while current_step is not None:
        next_step, next_flow = current_step.execute(context)

        if next_flow is not None:
            current_flow = config.flows[next_flow]

            if next_step is not None:
                current_step = current_flow.steps[next_step]
            else:
                current_step = current_flow.steps.get(current_flow.start_step)
        else:
            if next_step is not None:
                current_step = current_flow.steps[next_step]
            else:
                break
