import logging
import time
from pathlib import Path
from typing import TypedDict

import torch
from transformers import AutoTokenizer, PreTrainedModel, PreTrainedTokenizer, BertForSequenceClassification, pipeline, \
    Pipeline

from system.common.config import CompilerConfigV2, Interaction, Reply

# Initialize logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class Prediction(TypedDict):
    """
    Represents a prediction with a label and its associated score.

    Detailed description of the class, its purpose, and usage.

    :ivar label: The label or category of the prediction.
    :type label: str
    :ivar prediction: The confidence score or probability of the prediction.
    :type prediction: float
    """
    label: str
    prediction: float


class BertModelComponents:
    """
    Represents a container for a pre-trained model, tokenizer, and classification pipeline.

    This class holds components required for text/classification tasks using
    pre-trained models. It stores the model, tokenizer, and classification pipeline,
    providing functionality for text classification.

    :ivar model: The pre-trained model used for performing the text classification.
    :type model: PreTrainedModel
    :ivar tokenizer: The tokenizer associated with the given pre-trained model.
    :type tokenizer: PreTrainedTokenizer
    :ivar classifier: The classification pipeline that uses the model and tokenizer
        to perform text classification.
    :type classifier: Pipeline
    """
    def __init__(self, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, classifier: Pipeline):
        self.model = model
        self.tokenizer = tokenizer
        self.classifier = classifier

    def classify(self, text: str) -> list[Prediction]:
        """
        Classifies the given text using the initialized model and returns a list of
        predictions. The function uses a pre-trained classifier model to analyze
        the input text and generate predictions based on it. The duration of the
        classification process is logged for debugging purposes.

        :param text: The input text to be classified.
        :type text: str
        :return: A list of predictions generated by the classifier for the input text.
        :rtype: list[Prediction]
        """
        logger.debug(f"Using model: {self.model.config.name_or_path} to classify: \"{text}\"...")

        start_time = time.time()
        # The Pipeline object implements __call__, so this is valid.
        # The object returned is
        predictions: list[Prediction] = self.classifier(text)[0]
        end_time = time.time()

        logger.debug(f"Classification took {end_time - start_time} seconds. It returned: {predictions}")

        return predictions

    def __repr__(self):
        return f"ModelTokenizerPair(model={self.model}, tokenizer={self.tokenizer})"

def load_models(conf_artifact_path: Path, root_interaction: Interaction) -> dict[str, BertModelComponents]:
    """
    Loads all the necessary pre-trained models and their respective components
    like the tokenizer and classifier pipeline for text classification. This
    function identifies the models required based on the root interaction, loads
    each model, and handles any errors encountered during the loading process.
    If any critical error occurs during model loading, it halts the startup
    process.

    :param conf_artifact_path: Path to the configuration artifact directory where
        model directories are located.
    :type conf_artifact_path: Path
    :param root_interaction: An interaction object that helps to determine
        the models/resources that need to be loaded.
    :type root_interaction: Interaction
    :return: A dictionary mapping model names to their corresponding components
        (model, tokenizer, classifier pipeline).
    :rtype: dict[str, BertModelComponents]
    :raises RuntimeError: If some or all models fail to load successfully,
        halting the process with an error message.
    """
    def load_model(model_name: str, conf_artifact_path: Path, subdir: str = "trained_model") -> BertModelComponents:
        """
        Loads a pre-trained model, tokenizer, and classifier for text classification from the specified
        configuration artifact path and subdirectory. Returns the components wrapped in a ModelComponents
        object. The function handles the initialization of the model and tokenizer using their respective
        libraries, and sets up a text classification pipeline.

        :param model_name: The name of the model which is used to locate its files inside the configuration
            artifact directory.
        :type model_name: str

        :param conf_artifact_path: The base path to the directory containing the configuration artifacts
            for multiple models.
        :type conf_artifact_path: Path

        :param subdir: The subdirectory inside the model directory that contains the trained model data.
            Defaults to "trained_model".
        :type subdir: str

        :return: A ModelComponents object containing the loaded model, tokenizer, and classifier pipeline.
        :rtype: BertModelComponents
        """
        try:
            path = conf_artifact_path / model_name / subdir
            model = BertForSequenceClassification.from_pretrained(path)
            tokenizer = AutoTokenizer.from_pretrained(path)
            classifier = pipeline(model=model, tokenizer=tokenizer, task="text-classification",
                                  top_k=None,  # by setting top_k to None we get all the predictions.
                                  device=torch.cuda.current_device())
            return BertModelComponents(model, tokenizer, classifier)
        except Exception as er:
            logger.error(f"Error loading model '{model_name}': {er}")
            raise

    required_models = root_interaction.discover_resources_to_load()
    models: dict[str, BertModelComponents] = {}
    errors = []

    for r in required_models:
        try:
            models[r] = load_model(r, conf_artifact_path)
        except Exception as e:
            errors.append(f"Model '{r}' failed to load: {e}")

    if errors:
        error_message = "\n".join(errors)
        logger.error(f"Failed to load all required models:\n{error_message}")
        raise RuntimeError(f"Startup process halted due to model loading errors:\n{error_message}")

    return models


def main():
    artifacts_dir = Path("../compiler/artifacts")
    config = CompilerConfigV2.load_from_file('../compiler/test_config v2.yml')
    conf_artifact_path = artifacts_dir / config.name

    models = load_models(conf_artifact_path, config.interaction)

    while True:
        user_input = input("Enter a prompt: ")
        if user_input == "exit":
            break

        next_interaction: Interaction | Reply = config.interaction
        interaction_traversal_stack: list[(str, Interaction | Reply)] = [("root", next_interaction)]

        while True:
            interaction_model = models[next_interaction.use]
            predictions = interaction_model.classify(user_input)
            next_interaction = next_interaction.cases[predictions[0]["label"]]

            interaction_traversal_stack.append((predictions, next_interaction))

            if isinstance(next_interaction, Reply):
                r: Reply = next_interaction

                stack = [(i[0], f"{type(i[1]).__name__}: {i[1].name}") for i in interaction_traversal_stack]
                logger.info(f"Stack: {stack}")

                print(f"Reply: {r.get_reply()}")
                break


if __name__ == '__main__':
    main()
