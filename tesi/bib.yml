unitotmplt:
  type: Repository
  title: "UniTO typst template"
  url: "https://github.com/eduardz1/UniTO-typst-template"

typst:
  type: Web
  title: "Typst"
  url: "http://typst.app"

eliza:
  type: article
  title: ELIZA—a computer program for the study of natural language communication between man and machine
  author: Weizenbaum, Joseph
  date: 1966-01
  page-range: 36-45
  url: https://doi.org/10.1145/365153.365168
  serial-number:
    doi: 10.1145/365153.365168
    issn: 0001-0782
  parent:
    type: periodical
    title: Commun. ACM
    publisher: Association for Computing Machinery
    location: New York, NY, USA
    issue: 1
    volume: 9

rogers:
  type: article
  title: The Necessary and Sufficient Conditions of Psychotherapeutic Personality Change
  author: ROGERS, CARL
  date: 2007
  page-range: 240-8
  serial-number:
    doi: 10.1037/0033-3204.44.3.240
  parent:
    type: periodical
    title: Psychotherapy (Chicago, Ill.)
    volume: 44

eliza_history:
  type: article
  title: "The computational therapeutic: exploring Weizenbaum's ELIZA as a history of the present"
  author: Bassett, Caroline
  date: 2019-12-01
  page-range: 803-812
  url: https://doi.org/10.1007/s00146-018-0825-9
  serial-number:
    doi: 10.1007/s00146-018-0825-9
    issn: 1435-5655
  abstract: This paper explores the history of ELIZA, a computer programme approximating a Rogerian therapist, developed by Jospeh Weizenbaum at MIT in the 1970s, as an early AI experiment. ELIZA's reception provoked Weizenbaum to re-appraise the relationship between `computer power and human reason' and to attack the `powerful delusional thinking' about computers and their intelligence that he understood to be widespread in the general public and also amongst experts. The root issue for Weizenbaum was whether human thought could be `entirely computable' (reducible to logical formalism). This also provoked him to re-consider the nature of machine intelligence and to question the instantiation of its logics in the social world, which would come to operate, he said, as a `slow acting poison'. Exploring Weizenbaum's 20th Century apostasy, in the light of ELIZA, illustrates ways in which contemporary anxieties and debates over machine smartness connect to earlier formations. In particular, this article argues that it is in its designation as a computational therapist that ELIZA is most significant today. ELIZA points towards a form of human-machine relationship now pervasive, a precursor of the `machinic therapeutic' condition we find ourselves in, and thus speaks very directly to questions concerning modulation, autonomy, and the new behaviorism that are currently arising.
  parent:
    type: periodical
    title: AI {&} SOCIETY
    issue: 4
    volume: 34

jm3:
  type: book
  title: "Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition with Language Models"
  author:
    - Jurafsky, Daniel
    - Martin, James H.
  date: 2025
  edition: 3rd
  url: https://web.stanford.edu/~jurafsky/slp3/

ABRAHAMS196951:
  type: anthos
  title: Symbol Manipulation Languages
  author: Abrahams, Paul W.
  date: 1969
  editor:
    - Alt, Franz L.
    - Rubinoff, Morris
  page-range: 51-111
  url: https://www.sciencedirect.com/science/article/pii/S0065245808603113
  serial-number:
    doi: https://doi.org/10.1016/S0065-2458(08)60311-3
    issn: 0065-2458
  abstract: "Publisher Summary Symbol manipulation is a branch of computing concerned with the manipulation of unpredictably structured data. Symbol manipulation languages vary in the generality of the lists upon which they operate. The development of such languages from IPL-V through LISP, L6, PL/l, SLIP, SNOBOL, COMIT, and EOL with comments on their relative capabilities, advantages, and disadvantages are presented. These languages may arise either through the embedding of symbol manipulation facilities in a general-purpose language or through the expansion of a symbol manipulation language to include general computation. L6 and EOL are low-level languages. Their simplicity contrasts with the complexity of the higher-level languages. The symbol manipulation must meet two requirements: (1) representation of lists and (2) language features, are also discussed. Pattern-matching is a recurrent theme in symbol manipulation languages. Symbol manipulation is a rapidly expanding branch of computing. It appears likely that symbol manipulation languages will stabilize as their use becomes more widespread, in much the same way as scientific and commercial languages have stabilized."
  parent:
    type: anthology
    publisher: Elsevier
    volume: 9
    parent:
      type: anthology
      title: Advances in Computers

imitation_game:
  type: article
  title: I.—COMPUTING MACHINERY AND INTELLIGENCE
  author: TURING, A. M.
  date: 1950
  page-range: 433-460
  url: https://doi.org/10.1093/mind/LIX.236.433
  serial-number:
    doi: 10.1093/mind/LIX.236.433
    issn: 0026-4423
  parent:
    type: periodical
    title: Mind
    issue: 236
    volume: 59

alice:
  type: chapter
  title: The anatomy of A.L.I.C.E
  author: Wallace, Richard
  date: 2009
  page-range: 181-210
  serial-number:
    doi: 10.1007/978-1-4020-6710-5_13
    isbn: 978-1-4020-9624-2
  parent:
    type: book

aiml:
  type: Web
  title: Artificial Intelligence Markup Language
  organization: AIML Foundation
  url: http://www.aiml.foundation/doc.html

pandas:
  type: repository
  title: pandas - Python Data Analysis Library
  url: https://pandas.pydata.org/

rnn:
  type: chapter
  title: Learning internal representations by error propagation
  author:
    - Rumelhart, D. E.
    - Hinton, G. E.
    - Williams, R. J.
  date: 1986
  page-range: 318-362
  serial-number:
    isbn: 026268053X
  parent:
    type: book
    title: "Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1: Foundations"
    publisher: MIT Press
    location: Cambridge, MA, USA

rnn_intro:
  type: misc
  title: "Recurrent Neural Networks (RNNs): A gentle Introduction and Overview"
  author: Schmidt, Robin M.
  date: 2019
  url: https://arxiv.org/abs/1912.05911
  serial-number:
    arxiv: "1912.05911"

colah:
  type: web
  author: Christopher Olah
  title: "Understanding LSTM Networks"
  url: https://colah.github.io/posts/2015-08-Understanding-LSTMs/

rnn_deutch:
  type: article
  title: Untersuchungen zu dynamischen neuronalen Netzen
  author: Hochreiter, Sepp
  date: 1991
  page-range: ""
  parent:
    type: periodical

rnn_difficult:
  type: article
  title: Learning long-term dependencies with gradient descent is difficult
  author:
    - Bengio, Y.
    - Simard, P.
    - Frasconi, P.
  date: 1994-03
  page-range: 157-166
  serial-number:
    doi: 10.1109/72.279181
    issn: 1941-0093
  abstract: Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered.<>
  parent:
    type: periodical
    title: IEEE Transactions on Neural Networks
    issue: 2
    volume: 5

lstm:
  type: article
  title: Long Short-Term Memory
  author:
    - Hochreiter, Sepp
    - Schmidhuber, Jürgen
  date: 1997
  page-range: 1735-1780
  url: https://doi.org/10.1162/neco.1997.9.8.1735
  serial-number:
    doi: 10.1162/neco.1997.9.8.1735
    issn: 0899-7667
  abstract: Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.
  parent:
    type: periodical
    title: Neural Computation
    issue: 8
    volume: 9

LINDEMANN2021650:
  type: article
  title: A survey on long short-term memory networks for time series prediction
  author:
    - Lindemann, Benjamin
    - Müller, Timo
    - Vietz, Hannes
    - Jazdi, Nasser
    - Weyrich, Michael
  date: 2021
  page-range: 650-655
  url: https://www.sciencedirect.com/science/article/pii/S2212827121003796
  serial-number:
    doi: https://doi.org/10.1016/j.procir.2021.03.088
    issn: 2212-8271
  abstract: Recurrent neural networks and exceedingly Long short-term memory (LSTM) have been investigated intensively in recent years due to their ability to model and predict nonlinear time-variant system dynamics. The present paper delivers a comprehensive overview of existing LSTM cell derivatives and network architectures for time series prediction. A categorization in LSTM with optimized cell state representations and LSTM with interacting cell states is proposed. The investigated approaches are evaluated against defined requirements being relevant for an accurate time series prediction. These include short-term and long-term memory behavior, the ability for multimodal and multi-step ahead predictions and the according error propagation. Sequence-to-sequence networks with partially conditioning outperform the other approaches, such as bidirectional or associative networks, and are best suited to fulfill the requirements.
  parent:
    type: periodical
    title: Procedia CIRP
    volume: 99

google_voice:
  type: article
  title: "Neon prescription... or rather, New transcription for Google Voice"
  organization: Google Research
  url:
    value: https://blog.google/products/google-voice/neon-prescription-or-rather-new/
    date: 2015-07-23

google_voice_transcription:
  type: article
  title: "The neural networks behind Google Voice transcription"
  organization: Google Research
  url:
    value: https://research.google/blog/the-neural-networks-behind-google-voice-transcription/
    date: 2015-08-11

google_voice_accuracy:
  type: article
  title: "Google voice search: faster and more accurate"
  author:
    - Sak, Haşim
    - Senior, Andrew
    - Rao, Kanishka
    - Beaufays, Françoise
    - Schalkwyk, Johan
  publisher: Google Speech Team
  organization: Google Research
  url:
    value: https://research.google/blog/google-voice-search-faster-and-more-accurate/
    date: 2015-09-24

wu2016googlesneuralmachinetranslation:
  type: misc
  title: "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"
  author:
    - Wu, Yonghui
    - Schuster, Mike
    - Chen, Zhifeng
    - Le, Quoc V.
    - Norouzi, Mohammad
    - Macherey, Wolfgang
    - Krikun, Maxim
    - Cao, Yuan
    - Gao, Qin
    - Macherey, Klaus
    - Klingner, Jeff
    - Shah, Apurva
    - Johnson, Melvin
    - Liu, Xiaobing
    - Kaiser, Łukasz
    - Gouws, Stephan
    - Kato, Yoshikiyo
    - Kudo, Taku
    - Kazawa, Hideto
    - Stevens, Keith
    - Kurian, George
    - Patil, Nishant
    - Wang, Wei
    - Young, Cliff
    - Smith, Jason
    - Riesa, Jason
    - Rudnick, Alex
    - Vinyals, Oriol
    - Corrado, Greg
    - Hughes, Macduff
    - Dean, Jeffrey
  date: 2016
  url: https://arxiv.org/abs/1609.08144
  serial-number:
    arxiv: "1609.08144"

google_translate:
  type: article
  title: "A Neural Network for Machine Translation, at Production Scale"
  author:
    - Quoc V. Le
    - Mike Schuster
  publisher: Google Brain Team
  organization: Google Research
  url:
    value: https://research.google/blog/a-neural-network-for-machine-translation-at-production-scale/
    date: 2016-09-27

apple_lstm:
  type: article
  title: "Can Global Semantic Context Improve Neural Language Models?"
  publisher: Frameworks Natural Language Processing Team
  organization: Apple
  url: https://machinelearning.apple.com/research/can-global-semantic-context-improve-neural-language-models

alexa_lstm:
  type: article
  title: Bringing the Magic of Amazon AI and Alexa to Apps on AWS
  url: https://www.allthingsdistributed.com/2016/11/amazon-ai-and-alexa-for-all-aws-apps.html

facebook_lstm:
  type: article
  title: "Transitioning entirely to neural machine translation"
  author: Alexander Sidorov
  organization: Facebook
  url: https://engineering.fb.com/2017/08/03/ml-applications/transitioning-entirely-to-neural-machine-translation/

microsoft_lstm:
  type: article
  title: The Microsoft 2017 Conversational Speech Recognition System
  author:
    - Xiong, W.
    - Wu, L.
    - Alleva, F.
    - Droppo, J.
    - Huang, X.
    - Stolcke, A.
  date: 2018-04
  page-range: 5934-5938
  serial-number:
    doi: 10.1109/ICASSP.2018.8461870
    issn: 2379-190X
  abstract: We describe the latest version of Microsoft's conversational speech recognition system for the Switchboard and CallHome domains. The system adds a CNN-BLSTM acoustic model to the set of model architectures we combined previously, and includes character-based and dialog session aware LSTM language models in rescoring. For system combination we adopt a two-stage approach, whereby acoustic model posteriors are first combined at the senone/frame level, followed by a word-level voting via confusion networks. We also added another language model rescoring step following the confusion network combination. The resulting system yields a 5.1% word error rate on the NIST 2000 Switchboard test set, and 9.8% on the CallHome subset.
  parent:
    type: proceedings
    title: 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
    issue: ""
    volume: 0

openai_dota:
  type: article
  title: "The Science Behind OpenAI Five that just Produced One of the Greatest Breakthrough in the History of AI"
  url: https://web.archive.org/web/20191226222000/https://towardsdatascience.com/the-science-behind-openai-five-that-just-produced-one-of-the-greatest-breakthrough-in-the-history-b045bcdc2b69?gi=24b20ef8ca3f

learning_dexterity:
  type: article
  title: "Learning Dexterity"
  abstract: "We’ve trained a human-like robot hand to manipulate physical objects with unprecedented dexterity."
  organization: OpenAI
  url: https://openai.com/index/learning-dexterity/

deepmind_starcraft:
  type: article
  organization: "The AlphaStar Team"
  url: https://deepmind.google/discover/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning/

schmidhuber2022deeplearningmiraculousyear:
  type: misc
  title: "Deep Learning: Our Miraculous Year 1990-1991"
  author: Schmidhuber, Juergen
  date: 2022
  url: https://arxiv.org/abs/2005.05744
  serial-number:
    arxiv: "2005.05744"

vaswani2023attentionneed:
  type: misc
  title: Attention Is All You Need
  author:
    - Vaswani, Ashish
    - Shazeer, Noam
    - Parmar, Niki
    - Uszkoreit, Jakob
    - Jones, Llion
    - Gomez, Aidan N.
    - Kaiser, Lukasz
    - Polosukhin, Illia
  date: 2023
  serial-number:
    doi: "10.48550/arXiv.1706.03762"

bert:
  type: misc
  title: "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
  author:
    - Devlin, Jacob
    - Chang, Ming-Wei
    - Lee, Kenton
    - Toutanova, Kristina
  date: 2019
  serial-number:
    doi: 10.48550/arXiv.1810.04805

bbc_google_fails:
  type: article
  title: "Glue pizza and eat rocks: Google AI search errors go viral"
  author:
    - McMahon, Liv
    - Kleinman, Zoe
  url: https://www.bbc.com/news/articles/cd11gzejgz4o

distilbert:
  type: article
  title: "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"
  author:
    - Sanh, Victor
    - Debut, Lysandre
    - Chaumond, Julien
    - Wolf, Thomas
  date: 2020
  serial-number:
    doi: 10.48550/arXiv.1910.01108

hinton-distillation:
  type: article
  title: Distilling the Knowledge in a Neural Network
  author:
    - Hinton, Geoffrey
    - Vinyals, Oriol
    - Dean, Jeff
  date: 2015
  serial-number:
    doi: 10.48550/arXiv.1503.02531

mobilebert-uncased:
  type: repository
  url: https://huggingface.co/google/mobilebert-uncased
  title: "MobileBERT on Huggingface"

mobilebert:
  type: article
  title: "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices"
  author:
    - Sun, Zhiqing
    - Yu, Hongkun
    - Song, Xiaodan
    - Liu, Renjie
    - Yang, Yiming
    - Zhou, Denny
  date: 2020
  serial-number:
    doi: 10.48550/arXiv.2004.02984

electra-hf:
  type: repository
  title: "Electra on Huggingface"
  url: https://huggingface.co/google/electra-small-discriminator

adversarial-nets:
  type: misc
  title: Generative Adversarial Networks
  author:
    - Goodfellow, Ian J.
    - Pouget-Abadie, Jean
    - Mirza, Mehdi
    - Xu, Bing
    - Warde-Farley, David
    - Ozair, Sherjil
    - Courville, Aaron
    - Bengio, Yoshua
  date: 2014
  serial-number:
    doi: 10.48550/arXiv.1406.2661

deepseek-r1:
  type: misc
  title: "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
  author:
    - DeepSeek-AI
    - Guo, Daya
    - Yang, Dejian
    - Zhang, Haowei
    - Song, Junxiao
    - Zhang, Ruoyu
    - Xu, Runxin
    - Zhu, Qihao
    - Ma, Shirong
    - Wang, Peiyi
    - Bi, Xiao
    - Zhang, Xiaokang
    - Yu, Xingkai
    - Wu, Yu
    - Wu, Z. F.
    - Gou, Zhibin
    - Shao, Zhihong
    - Li, Zhuoshu
    - Gao, Ziyi
    - Liu, Aixin
    - Xue, Bing
    - Wang, Bingxuan
    - Wu, Bochao
    - Feng, Bei
    - Lu, Chengda
    - Zhao, Chenggang
    - Deng, Chengqi
    - Zhang, Chenyu
    - Ruan, Chong
    - Dai, Damai
    - Chen, Deli
    - Ji, Dongjie
    - Li, Erhang
    - Lin, Fangyun
    - Dai, Fucong
    - Luo, Fuli
    - Hao, Guangbo
    - Chen, Guanting
    - Li, Guowei
    - Zhang, H.
    - Bao, Han
    - Xu, Hanwei
    - Wang, Haocheng
    - Ding, Honghui
    - Xin, Huajian
    - Gao, Huazuo
    - Qu, Hui
    - Li, Hui
    - Guo, Jianzhong
    - Li, Jiashi
    - Wang, Jiawei
    - Chen, Jingchang
    - Yuan, Jingyang
    - Qiu, Junjie
    - Li, Junlong
    - Cai, J. L.
    - Ni, Jiaqi
    - Liang, Jian
    - Chen, Jin
    - Dong, Kai
    - Hu, Kai
    - Gao, Kaige
    - Guan, Kang
    - Huang, Kexin
    - Yu, Kuai
    - Wang, Lean
    - Zhang, Lecong
    - Zhao, Liang
    - Wang, Litong
    - Zhang, Liyue
    - Xu, Lei
    - Xia, Leyi
    - Zhang, Mingchuan
    - Zhang, Minghua
    - Tang, Minghui
    - Li, Meng
    - Wang, Miaojun
    - Li, Mingming
    - Tian, Ning
    - Huang, Panpan
    - Zhang, Peng
    - Wang, Qiancheng
    - Chen, Qinyu
    - Du, Qiushi
    - Ge, Ruiqi
    - Zhang, Ruisong
    - Pan, Ruizhe
    - Wang, Runji
    - Chen, R. J.
    - Jin, R. L.
    - Chen, Ruyi
    - Lu, Shanghao
    - Zhou, Shangyan
    - Chen, Shanhuang
    - Ye, Shengfeng
    - Wang, Shiyu
    - Yu, Shuiping
    - Zhou, Shunfeng
    - Pan, Shuting
    - Li, S. S.
    - Zhou, Shuang
    - Wu, Shaoqing
    - Ye, Shengfeng
    - Yun, Tao
    - Pei, Tian
    - Sun, Tianyu
    - Wang, T.
    - Zeng, Wangding
    - Zhao, Wanjia
    - Liu, Wen
    - Liang, Wenfeng
    - Gao, Wenjun
    - Yu, Wenqin
    - Zhang, Wentao
    - Xiao, W. L.
    - An, Wei
    - Liu, Xiaodong
    - Wang, Xiaohan
    - Chen, Xiaokang
    - Nie, Xiaotao
    - Cheng, Xin
    - Liu, Xin
    - Xie, Xin
    - Liu, Xingchao
    - Yang, Xinyu
    - Li, Xinyuan
    - Su, Xuecheng
    - Lin, Xuheng
    - Li, X. Q.
    - Jin, Xiangyue
    - Shen, Xiaojin
    - Chen, Xiaosha
    - Sun, Xiaowen
    - Wang, Xiaoxiang
    - Song, Xinnan
    - Zhou, Xinyi
    - Wang, Xianzu
    - Shan, Xinxia
    - Li, Y. K.
    - Wang, Y. Q.
    - Wei, Y. X.
    - Zhang, Yang
    - Xu, Yanhong
    - Li, Yao
    - Zhao, Yao
    - Sun, Yaofeng
    - Wang, Yaohui
    - Yu, Yi
    - Zhang, Yichao
    - Shi, Yifan
    - Xiong, Yiliang
    - He, Ying
    - Piao, Yishi
    - Wang, Yisong
    - Tan, Yixuan
    - Ma, Yiyang
    - Liu, Yiyuan
    - Guo, Yongqiang
    - Ou, Yuan
    - Wang, Yuduan
    - Gong, Yue
    - Zou, Yuheng
    - He, Yujia
    - Xiong, Yunfan
    - Luo, Yuxiang
    - You, Yuxiang
    - Liu, Yuxuan
    - Zhou, Yuyang
    - Zhu, Y. X.
    - Xu, Yanhong
    - Huang, Yanping
    - Li, Yaohui
    - Zheng, Yi
    - Zhu, Yuchen
    - Ma, Yunxian
    - Tang, Ying
    - Zha, Yukun
    - Yan, Yuting
    - Ren, Z. Z.
    - Ren, Zehui
    - Sha, Zhangli
    - Fu, Zhe
    - Xu, Zhean
    - Xie, Zhenda
    - Zhang, Zhengyan
    - Hao, Zhewen
    - Ma, Zhicheng
    - Yan, Zhigang
    - Wu, Zhiyu
    - Gu, Zihui
    - Zhu, Zijia
    - Liu, Zijun
    - Li, Zilin
    - Xie, Ziwei
    - Song, Ziyang
    - Pan, Zizheng
    - Huang, Zhen
    - Xu, Zhipeng
    - Zhang, Zhongyu
    - Zhang, Zhen
  date: 2025
  serial-number:
    doi: 10.48550/arXiv.2501.12948

deepseek-technical:
  type: misc
  title: DeepSeek-V3 Technical Report
  author:
    - DeepSeek-AI
    - Liu, Aixin
    - Feng, Bei
    - Xue, Bing
    - Wang, Bingxuan
    - Wu, Bochao
    - Lu, Chengda
    - Zhao, Chenggang
    - Deng, Chengqi
    - Zhang, Chenyu
    - Ruan, Chong
    - Dai, Damai
    - Guo, Daya
    - Yang, Dejian
    - Chen, Deli
    - Ji, Dongjie
    - Li, Erhang
    - Lin, Fangyun
    - Dai, Fucong
    - Luo, Fuli
    - Hao, Guangbo
    - Chen, Guanting
    - Li, Guowei
    - Zhang, H.
    - Bao, Han
    - Xu, Hanwei
    - Wang, Haocheng
    - Zhang, Haowei
    - Ding, Honghui
    - Xin, Huajian
    - Gao, Huazuo
    - Li, Hui
    - Qu, Hui
    - Cai, J. L.
    - Liang, Jian
    - Guo, Jianzhong
    - Ni, Jiaqi
    - Li, Jiashi
    - Wang, Jiawei
    - Chen, Jin
    - Chen, Jingchang
    - Yuan, Jingyang
    - Qiu, Junjie
    - Li, Junlong
    - Song, Junxiao
    - Dong, Kai
    - Hu, Kai
    - Gao, Kaige
    - Guan, Kang
    - Huang, Kexin
    - Yu, Kuai
    - Wang, Lean
    - Zhang, Lecong
    - Xu, Lei
    - Xia, Leyi
    - Zhao, Liang
    - Wang, Litong
    - Zhang, Liyue
    - Li, Meng
    - Wang, Miaojun
    - Zhang, Mingchuan
    - Zhang, Minghua
    - Tang, Minghui
    - Li, Mingming
    - Tian, Ning
    - Huang, Panpan
    - Wang, Peiyi
    - Zhang, Peng
    - Wang, Qiancheng
    - Zhu, Qihao
    - Chen, Qinyu
    - Du, Qiushi
    - Chen, R. J.
    - Jin, R. L.
    - Ge, Ruiqi
    - Zhang, Ruisong
    - Pan, Ruizhe
    - Wang, Runji
    - Xu, Runxin
    - Zhang, Ruoyu
    - Chen, Ruyi
    - Li, S. S.
    - Lu, Shanghao
    - Zhou, Shangyan
    - Chen, Shanhuang
    - Wu, Shaoqing
    - Ye, Shengfeng
    - Ye, Shengfeng
    - Ma, Shirong
    - Wang, Shiyu
    - Zhou, Shuang
    - Yu, Shuiping
    - Zhou, Shunfeng
    - Pan, Shuting
    - Wang, T.
    - Yun, Tao
    - Pei, Tian
    - Sun, Tianyu
    - Xiao, W. L.
    - Zeng, Wangding
    - Zhao, Wanjia
    - An, Wei
    - Liu, Wen
    - Liang, Wenfeng
    - Gao, Wenjun
    - Yu, Wenqin
    - Zhang, Wentao
    - Li, X. Q.
    - Jin, Xiangyue
    - Wang, Xianzu
    - Bi, Xiao
    - Liu, Xiaodong
    - Wang, Xiaohan
    - Shen, Xiaojin
    - Chen, Xiaokang
    - Zhang, Xiaokang
    - Chen, Xiaosha
    - Nie, Xiaotao
    - Sun, Xiaowen
    - Wang, Xiaoxiang
    - Cheng, Xin
    - Liu, Xin
    - Xie, Xin
    - Liu, Xingchao
    - Yu, Xingkai
    - Song, Xinnan
    - Shan, Xinxia
    - Zhou, Xinyi
    - Yang, Xinyu
    - Li, Xinyuan
    - Su, Xuecheng
    - Lin, Xuheng
    - Li, Y. K.
    - Wang, Y. Q.
    - Wei, Y. X.
    - Zhu, Y. X.
    - Zhang, Yang
    - Xu, Yanhong
    - Xu, Yanhong
    - Huang, Yanping
    - Li, Yao
    - Zhao, Yao
    - Sun, Yaofeng
    - Li, Yaohui
    - Wang, Yaohui
    - Yu, Yi
    - Zheng, Yi
    - Zhang, Yichao
    - Shi, Yifan
    - Xiong, Yiliang
    - He, Ying
    - Tang, Ying
    - Piao, Yishi
    - Wang, Yisong
    - Tan, Yixuan
    - Ma, Yiyang
    - Liu, Yiyuan
    - Guo, Yongqiang
    - Wu, Yu
    - Ou, Yuan
    - Zhu, Yuchen
    - Wang, Yuduan
    - Gong, Yue
    - Zou, Yuheng
    - He, Yujia
    - Zha, Yukun
    - Xiong, Yunfan
    - Ma, Yunxian
    - Yan, Yuting
    - Luo, Yuxiang
    - You, Yuxiang
    - Liu, Yuxuan
    - Zhou, Yuyang
    - Wu, Z. F.
    - Ren, Z. Z.
    - Ren, Zehui
    - Sha, Zhangli
    - Fu, Zhe
    - Xu, Zhean
    - Huang, Zhen
    - Zhang, Zhen
    - Xie, Zhenda
    - Zhang, Zhengyan
    - Hao, Zhewen
    - Gou, Zhibin
    - Ma, Zhicheng
    - Yan, Zhigang
    - Shao, Zhihong
    - Xu, Zhipeng
    - Wu, Zhiyu
    - Zhang, Zhongyu
    - Li, Zhuoshu
    - Gu, Zihui
    - Zhu, Zijia
    - Liu, Zijun
    - Li, Zilin
    - Xie, Ziwei
    - Song, Ziyang
    - Gao, Ziyi
    - Pan, Zizheng
  date: 2025
  serial-number:
    doi: 10.48550/arXiv.2412.19437

openai-llm:
  type: misc
  title: Language Models are Few-Shot Learners
  author:
    - Brown, Tom B.
    - Mann, Benjamin
    - Ryder, Nick
    - Subbiah, Melanie
    - Kaplan, Jared
    - Dhariwal, Prafulla
    - Neelakantan, Arvind
    - Shyam, Pranav
    - Sastry, Girish
    - Askell, Amanda
    - Agarwal, Sandhini
    - Herbert-Voss, Ariel
    - Krueger, Gretchen
    - Henighan, Tom
    - Child, Rewon
    - Ramesh, Aditya
    - Ziegler, Daniel M.
    - Wu, Jeffrey
    - Winter, Clemens
    - Hesse, Christopher
    - Chen, Mark
    - Sigler, Eric
    - Litwin, Mateusz
    - Gray, Scott
    - Chess, Benjamin
    - Clark, Jack
    - Berner, Christopher
    - McCandlish, Sam
    - Radford, Alec
    - Sutskever, Ilya
    - Amodei, Dario
  date: 2020
  serial-number:
    doi: "10.48550/arXiv.2005.14165"

dont_stop_pretraining:
  type: article
  title: "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks"
  author:
    - Gururangan, Suchin
    - Marasović, Ana
    - Swayamdipta, Swabha
    - Lo, Kyle
    - Beltagy, Iz
    - Downey, Doug
    - Smith, Noah A.
  date: 2020
  url: https://arxiv.org/abs/2004.10964
  serial-number:
    arxiv: "2004.10964"

markdown:
  type: post
  title: "Markdown"
  author: John Gruber
  url: https://daringfireball.net/projects/markdown/
  abstract: "Markdown is a text-to-HTML conversion tool for web writers. Markdown allows you to write using an easy-to-read, easy-to-write plain text format, then convert it to structurally valid XHTML (or HTML)."

huggingface_transformers:
  type: article
  title: "HuggingFace's Transformers: State-of-the-art Natural Language Processing"
  author:
    - Wolf, Thomas
    - Debut, Lysandre
    - Sanh, Victor
    - Chaumond, Julien
    - Delangue, Clement
    - Moi, Anthony
    - Cistac, Pierric
    - Rault, Tim
    - Louf, Rémi
    - Funtowicz, Morgan
    - Davison, Joe
    - Shleifer, Sam
    - name: Platen
      given-name: Patrick
      prefix: von
    - Ma, Clara
    - Jernite, Yacine
    - Plu, Julien
    - Xu, Canwen
    - Scao, Teven Le
    - Gugger, Sylvain
    - Drame, Mariama
    - Lhoest, Quentin
    - Rush, Alexander M.
  date: 2020
  serial-number:
    doi: "10.48550/arXiv.1910.03771"

loebner:
  type: web
  title: The Loebner Prize
  organization: Berkeley University
  url: https://www.ocf.berkeley.edu/~arihuang/academic/research/loebner.html

bert_fine_tuning:
  type: article
  title: "On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines"
  author:
    - Mosbach, Marius
    - Andriushchenko, Maksym
    - Klakow, Dietrich
  date: 2021
  url: https://arxiv.org/abs/2006.04884
  serial-number:
    doi: "10.48550/arXiv.2006.04884"

ollama:
  title: Ollama - LLM local runner
  type: repository
  organization: Ollama
  url: https://github.com/ollama/ollama

electra:
  type: article
  title: "ELECTRA: Pre-training text encoders as discriminators rather than generators"
  organization: Google
  url: https://openreview.net/pdf?id=r1xMH1BtvB
  author:
    - Clark, Kevin
    - Luong, Minh-Thang
    - Le, Quoc V.
    - Manning, Christopher D.

bert-base:
  type: repository
  title: Bert uncased model by Google
  url: https://huggingface.co/google-bert/bert-base-uncased

distilbert-base:
  type: repository
  title: Distilbert base model
  url: https://huggingface.co/distilbert/distilbert-base-uncased

gemma:
  type: article
  title: "Gemma 2: Improving Open Language Models at a Practical Size"
  organization: Google Deep Mind
  author:
    - Team, Gemma
    - Riviere, Morgane
    - Pathak, Shreya
    - Sessa, Pier Giuseppe
    - Hardin, Cassidy
    - Bhupatiraju, Surya
    - Hussenot, Léonard
    - Mesnard, Thomas
    - Shahriari, Bobak
    - Ramé, Alexandre
    - Ferret, Johan
    - Liu, Peter
    - Tafti, Pouya
    - Friesen, Abe
    - Casbon, Michelle
    - Ramos, Sabela
    - Kumar, Ravin
    - Lan, Charline Le
    - Jerome, Sammy
    - Tsitsulin, Anton
    - Vieillard, Nino
    - Stanczyk, Piotr
    - Girgin, Sertan
    - Momchev, Nikola
    - Hoffman, Matt
    - Thakoor, Shantanu
    - Grill, Jean-Bastien
    - Neyshabur, Behnam
    - Bachem, Olivier
    - Walton, Alanna
    - Severyn, Aliaksei
    - Parrish, Alicia
    - Ahmad, Aliya
    - Hutchison, Allen
    - Abdagic, Alvin
    - Carl, Amanda
    - Shen, Amy
    - Brock, Andy
    - Coenen, Andy
    - Laforge, Anthony
    - Paterson, Antonia
    - Bastian, Ben
    - Piot, Bilal
    - Wu, Bo
    - Royal, Brandon
    - Chen, Charlie
    - Kumar, Chintu
    - Perry, Chris
    - Welty, Chris
    - Choquette-Choo, Christopher A.
    - Sinopalnikov, Danila
    - Weinberger, David
    - Vijaykumar, Dimple
    - Rogozińska, Dominika
    - Herbison, Dustin
    - Bandy, Elisa
    - Wang, Emma
    - Noland, Eric
    - Moreira, Erica
    - Senter, Evan
    - Eltyshev, Evgenii
    - Visin, Francesco
    - Rasskin, Gabriel
    - Wei, Gary
    - Cameron, Glenn
    - Martins, Gus
    - Hashemi, Hadi
    - Klimczak-Plucińska, Hanna
    - Batra, Harleen
    - Dhand, Harsh
    - Nardini, Ivan
    - Mein, Jacinda
    - Zhou, Jack
    - Svensson, James
    - Stanway, Jeff
    - Chan, Jetha
    - Zhou, Jin Peng
    - Carrasqueira, Joana
    - Iljazi, Joana
    - Becker, Jocelyn
    - Fernandez, Joe
    - name: Amersfoort
      given-name: Joost
      prefix: van
    - Gordon, Josh
    - Lipschultz, Josh
    - Newlan, Josh
    - Ji, Ju-yeong
    - Mohamed, Kareem
    - Badola, Kartikeya
    - Black, Kat
    - Millican, Katie
    - McDonell, Keelin
    - Nguyen, Kelvin
    - Sodhia, Kiranbir
    - Greene, Kish
    - Sjoesund, Lars Lowe
    - Usui, Lauren
    - Sifre, Laurent
    - Heuermann, Lena
    - Lago, Leticia
    - McNealus, Lilly
    - Soares, Livio Baldini
    - Kilpatrick, Logan
    - Dixon, Lucas
    - Martins, Luciano
    - Reid, Machel
    - Singh, Manvinder
    - Iverson, Mark
    - Görner, Martin
    - Velloso, Mat
    - Wirth, Mateo
    - Davidow, Matt
    - Miller, Matt
    - Rahtz, Matthew
    - Watson, Matthew
    - Risdal, Meg
    - Kazemi, Mehran
    - Moynihan, Michael
    - Zhang, Ming
    - Kahng, Minsuk
    - Park, Minwoo
    - Rahman, Mofi
    - Khatwani, Mohit
    - Dao, Natalie
    - Bardoliwalla, Nenshad
    - Devanathan, Nesh
    - Dumai, Neta
    - Chauhan, Nilay
    - Wahltinez, Oscar
    - Botarda, Pankil
    - Barnes, Parker
    - Barham, Paul
    - Michel, Paul
    - Jin, Pengchong
    - Georgiev, Petko
    - Culliton, Phil
    - Kuppala, Pradeep
    - Comanescu, Ramona
    - Merhej, Ramona
    - Jana, Reena
    - Rokni, Reza Ardeshir
    - Agarwal, Rishabh
    - Mullins, Ryan
    - Saadat, Samaneh
    - Carthy, Sara Mc
    - Cogan, Sarah
    - Perrin, Sarah
    - Arnold, Sébastien M. R.
    - Krause, Sebastian
    - Dai, Shengyang
    - Garg, Shruti
    - Sheth, Shruti
    - Ronstrom, Sue
    - Chan, Susan
    - Jordan, Timothy
    - Yu, Ting
    - Eccles, Tom
    - Hennigan, Tom
    - Kocisky, Tomas
    - Doshi, Tulsee
    - Jain, Vihan
    - Yadav, Vikas
    - Meshram, Vilobh
    - Dharmadhikari, Vishal
    - Barkley, Warren
    - Wei, Wei
    - Ye, Wenming
    - Han, Woohyun
    - Kwon, Woosuk
    - Xu, Xiang
    - Shen, Zhe
    - Gong, Zhitao
    - Wei, Zichuan
    - Cotruta, Victor
    - Kirk, Phoebe
    - Rao, Anand
    - Giang, Minh
    - Peran, Ludovic
    - Warkentin, Tris
    - Collins, Eli
    - Barral, Joelle
    - Ghahramani, Zoubin
    - Hadsell, Raia
    - Sculley, D.
    - Banks, Jeanine
    - Dragan, Anca
    - Petrov, Slav
    - Vinyals, Oriol
    - Dean, Jeff
    - Hassabis, Demis
    - Kavukcuoglu, Koray
    - Farabet, Clement
    - Buchatskaya, Elena
    - Borgeaud, Sebastian
    - Fiedel, Noah
    - Joulin, Armand
    - Kenealy, Kathleen
    - Dadashi, Robert
    - Andreev, Alek
  date: 2024
  serial-number:
    doi: "10.48550/arXiv.2408.00118"

llama3:
  type: article
  title: The Llama 3 Herd of Models
  organization: Meta
  author:
    - Grattafiori, Aaron
    - Dubey, Abhimanyu
    - Jauhri, Abhinav
    - Pandey, Abhinav
    - Kadian, Abhishek
    - Al-Dahle, Ahmad
    - Letman, Aiesha
    - Mathur, Akhil
    - Schelten, Alan
    - Vaughan, Alex
    - Yang, Amy
    - Fan, Angela
    - Goyal, Anirudh
    - Hartshorn, Anthony
    - Yang, Aobo
    - Mitra, Archi
    - Sravankumar, Archie
    - Korenev, Artem
    - Hinsvark, Arthur
    - Rao, Arun
    - Zhang, Aston
    - Rodriguez, Aurelien
    - Gregerson, Austen
    - Spataru, Ava
    - Roziere, Baptiste
    - Biron, Bethany
    - Tang, Binh
    - Chern, Bobbie
    - Caucheteux, Charlotte
    - Nayak, Chaya
    - Bi, Chloe
    - Marra, Chris
    - McConnell, Chris
    - Keller, Christian
    - Touret, Christophe
    - Wu, Chunyang
    - Wong, Corinne
    - Ferrer, Cristian Canton
    - Nikolaidis, Cyrus
    - Allonsius, Damien
    - Song, Daniel
    - Pintz, Danielle
    - Livshits, Danny
    - Wyatt, Danny
    - Esiobu, David
    - Choudhary, Dhruv
    - Mahajan, Dhruv
    - Garcia-Olano, Diego
    - Perino, Diego
    - Hupkes, Dieuwke
    - Lakomkin, Egor
    - AlBadawy, Ehab
    - Lobanova, Elina
    - Dinan, Emily
    - Smith, Eric Michael
    - Radenovic, Filip
    - Guzmán, Francisco
    - Zhang, Frank
    - Synnaeve, Gabriel
    - Lee, Gabrielle
    - Anderson, Georgia Lewis
    - Thattai, Govind
    - Nail, Graeme
    - Mialon, Gregoire
    - Pang, Guan
    - Cucurell, Guillem
    - Nguyen, Hailey
    - Korevaar, Hannah
    - Xu, Hu
    - Touvron, Hugo
    - Zarov, Iliyan
    - Ibarra, Imanol Arrieta
    - Kloumann, Isabel
    - Misra, Ishan
    - Evtimov, Ivan
    - Zhang, Jack
    - Copet, Jade
    - Lee, Jaewon
    - Geffert, Jan
    - Vranes, Jana
    - Park, Jason
    - Mahadeokar, Jay
    - Shah, Jeet
    - name: Linde
      given-name: Jelmer
      prefix: van der
    - Billock, Jennifer
    - Hong, Jenny
    - Lee, Jenya
    - Fu, Jeremy
    - Chi, Jianfeng
    - Huang, Jianyu
    - Liu, Jiawen
    - Wang, Jie
    - Yu, Jiecao
    - Bitton, Joanna
    - Spisak, Joe
    - Park, Jongsoo
    - Rocca, Joseph
    - Johnstun, Joshua
    - Saxe, Joshua
    - Jia, Junteng
    - Alwala, Kalyan Vasuden
    - Prasad, Karthik
    - Upasani, Kartikeya
    - Plawiak, Kate
    - Li, Ke
    - Heafield, Kenneth
    - Stone, Kevin
    - El-Arini, Khalid
    - Iyer, Krithika
    - Malik, Kshitiz
    - Chiu, Kuenley
    - Bhalla, Kunal
    - Lakhotia, Kushal
    - Rantala-Yeary, Lauren
    - name: Maaten
      given-name: Laurens
      prefix: van der
    - Chen, Lawrence
    - Tan, Liang
    - Jenkins, Liz
    - Martin, Louis
    - Madaan, Lovish
    - Malo, Lubo
    - Blecher, Lukas
    - Landzaat, Lukas
    - name: Oliveira
      given-name: Luke
      prefix: de
    - Muzzi, Madeline
    - Pasupuleti, Mahesh
    - Singh, Mannat
    - Paluri, Manohar
    - Kardas, Marcin
    - Tsimpoukelli, Maria
    - Oldham, Mathew
    - Rita, Mathieu
    - Pavlova, Maya
    - Kambadur, Melanie
    - Lewis, Mike
    - Si, Min
    - Singh, Mitesh Kumar
    - Hassan, Mona
    - Goyal, Naman
    - Torabi, Narjes
    - Bashlykov, Nikolay
    - Bogoychev, Nikolay
    - Chatterji, Niladri
    - Zhang, Ning
    - Duchenne, Olivier
    - Çelebi, Onur
    - Alrassy, Patrick
    - Zhang, Pengchuan
    - Li, Pengwei
    - Vasic, Petar
    - Weng, Peter
    - Bhargava, Prajjwal
    - Dubal, Pratik
    - Krishnan, Praveen
    - Koura, Punit Singh
    - Xu, Puxin
    - He, Qing
    - Dong, Qingxiao
    - Srinivasan, Ragavan
    - Ganapathy, Raj
    - Calderer, Ramon
    - Cabral, Ricardo Silveira
    - Stojnic, Robert
    - Raileanu, Roberta
    - Maheswari, Rohan
    - Girdhar, Rohit
    - Patel, Rohit
    - Sauvestre, Romain
    - Polidoro, Ronnie
    - Sumbaly, Roshan
    - Taylor, Ross
    - Silva, Ruan
    - Hou, Rui
    - Wang, Rui
    - Hosseini, Saghar
    - Chennabasappa, Sahana
    - Singh, Sanjay
    - Bell, Sean
    - Kim, Seohyun Sonia
    - Edunov, Sergey
    - Nie, Shaoliang
    - Narang, Sharan
    - Raparthy, Sharath
    - Shen, Sheng
    - Wan, Shengye
    - Bhosale, Shruti
    - Zhang, Shun
    - Vandenhende, Simon
    - Batra, Soumya
    - Whitman, Spencer
    - Sootla, Sten
    - Collot, Stephane
    - Gururangan, Suchin
    - Borodinsky, Sydney
    - Herman, Tamar
    - Fowler, Tara
    - Sheasha, Tarek
    - Georgiou, Thomas
    - Scialom, Thomas
    - Speckbacher, Tobias
    - Mihaylov, Todor
    - Xiao, Tong
    - Karn, Ujjwal
    - Goswami, Vedanuj
    - Gupta, Vibhor
    - Ramanathan, Vignesh
    - Kerkez, Viktor
    - Gonguet, Vincent
    - Do, Virginie
    - Vogeti, Vish
    - Albiero, Vítor
    - Petrovic, Vladan
    - Chu, Weiwei
    - Xiong, Wenhan
    - Fu, Wenyin
    - Meers, Whitney
    - Martinet, Xavier
    - Wang, Xiaodong
    - Wang, Xiaofang
    - Tan, Xiaoqing Ellen
    - Xia, Xide
    - Xie, Xinfeng
    - Jia, Xuchao
    - Wang, Xuewei
    - Goldschlag, Yaelle
    - Gaur, Yashesh
    - Babaei, Yasmine
    - Wen, Yi
    - Song, Yiwen
    - Zhang, Yuchen
    - Li, Yue
    - Mao, Yuning
    - Coudert, Zacharie Delpierre
    - Yan, Zheng
    - Chen, Zhengxing
    - Papakipos, Zoe
    - Singh, Aaditya
    - Srivastava, Aayushi
    - Jain, Abha
    - Kelsey, Adam
    - Shajnfeld, Adam
    - Gangidi, Adithya
    - Victoria, Adolfo
    - Goldstand, Ahuva
    - Menon, Ajay
    - Sharma, Ajay
    - Boesenberg, Alex
    - Baevski, Alexei
    - Feinstein, Allie
    - Kallet, Amanda
    - Sangani, Amit
    - Teo, Amos
    - Yunus, Anam
    - Lupu, Andrei
    - Alvarado, Andres
    - Caples, Andrew
    - Gu, Andrew
    - Ho, Andrew
    - Poulton, Andrew
    - Ryan, Andrew
    - Ramchandani, Ankit
    - Dong, Annie
    - Franco, Annie
    - Goyal, Anuj
    - Saraf, Aparajita
    - Chowdhury, Arkabandhu
    - Gabriel, Ashley
    - Bharambe, Ashwin
    - Eisenman, Assaf
    - Yazdan, Azadeh
    - James, Beau
    - Maurer, Ben
    - Leonhardi, Benjamin
    - Huang, Bernie
    - Loyd, Beth
    - Paola, Beto De
    - Paranjape, Bhargavi
    - Liu, Bing
    - Wu, Bo
    - Ni, Boyu
    - Hancock, Braden
    - Wasti, Bram
    - Spence, Brandon
    - Stojkovic, Brani
    - Gamido, Brian
    - Montalvo, Britt
    - Parker, Carl
    - Burton, Carly
    - Mejia, Catalina
    - Liu, Ce
    - Wang, Changhan
    - Kim, Changkyu
    - Zhou, Chao
    - Hu, Chester
    - Chu, Ching-Hsiang
    - Cai, Chris
    - Tindal, Chris
    - Feichtenhofer, Christoph
    - Gao, Cynthia
    - Civin, Damon
    - Beaty, Dana
    - Kreymer, Daniel
    - Li, Daniel
    - Adkins, David
    - Xu, David
    - Testuggine, Davide
    - David, Delia
    - Parikh, Devi
    - Liskovich, Diana
    - Foss, Didem
    - Wang, Dingkang
    - Le, Duc
    - Holland, Dustin
    - Dowling, Edward
    - Jamil, Eissa
    - Montgomery, Elaine
    - Presani, Eleonora
    - Hahn, Emily
    - Wood, Emily
    - Le, Eric-Tuan
    - Brinkman, Erik
    - Arcaute, Esteban
    - Dunbar, Evan
    - Smothers, Evan
    - Sun, Fei
    - Kreuk, Felix
    - Tian, Feng
    - Kokkinos, Filippos
    - Ozgenel, Firat
    - Caggioni, Francesco
    - Kanayet, Frank
    - Seide, Frank
    - Florez, Gabriela Medina
    - Schwarz, Gabriella
    - Badeer, Gada
    - Swee, Georgia
    - Halpern, Gil
    - Herman, Grant
    - Sizov, Grigory
    - Guangyi
    - Zhang
    - Lakshminarayanan, Guna
    - Inan, Hakan
    - Shojanazeri, Hamid
    - Zou, Han
    - Wang, Hannah
    - Zha, Hanwen
    - Habeeb, Haroun
    - Rudolph, Harrison
    - Suk, Helen
    - Aspegren, Henry
    - Goldman, Hunter
    - Zhan, Hongyuan
    - Damlaj, Ibrahim
    - Molybog, Igor
    - Tufanov, Igor
    - Leontiadis, Ilias
    - Veliche, Irina-Elena
    - Gat, Itai
    - Weissman, Jake
    - Geboski, James
    - Kohli, James
    - Lam, Janice
    - Asher, Japhet
    - Gaya, Jean-Baptiste
    - Marcus, Jeff
    - Tang, Jeff
    - Chan, Jennifer
    - Zhen, Jenny
    - Reizenstein, Jeremy
    - Teboul, Jeremy
    - Zhong, Jessica
    - Jin, Jian
    - Yang, Jingyi
    - Cummings, Joe
    - Carvill, Jon
    - Shepard, Jon
    - McPhie, Jonathan
    - Torres, Jonathan
    - Ginsburg, Josh
    - Wang, Junjie
    - Wu, Kai
    - U, Kam Hou
    - Saxena, Karan
    - Khandelwal, Kartikay
    - Zand, Katayoun
    - Matosich, Kathy
    - Veeraraghavan, Kaushik
    - Michelena, Kelly
    - Li, Keqian
    - Jagadeesh, Kiran
    - Huang, Kun
    - Chawla, Kunal
    - Huang, Kyle
    - Chen, Lailin
    - Garg, Lakshya
    - A, Lavender
    - Silva, Leandro
    - Bell, Lee
    - Zhang, Lei
    - Guo, Liangpeng
    - Yu, Licheng
    - Moshkovich, Liron
    - Wehrstedt, Luca
    - Khabsa, Madian
    - Avalani, Manav
    - Bhatt, Manish
    - Mankus, Martynas
    - Hasson, Matan
    - Lennie, Matthew
    - Reso, Matthias
    - Groshev, Maxim
    - Naumov, Maxim
    - Lathi, Maya
    - Keneally, Meghan
    - Liu, Miao
    - Seltzer, Michael L.
    - Valko, Michal
    - Restrepo, Michelle
    - Patel, Mihir
    - Vyatskov, Mik
    - Samvelyan, Mikayel
    - Clark, Mike
    - Macey, Mike
    - Wang, Mike
    - Hermoso, Miquel Jubert
    - Metanat, Mo
    - Rastegari, Mohammad
    - Bansal, Munish
    - Santhanam, Nandhini
    - Parks, Natascha
    - White, Natasha
    - Bawa, Navyata
    - Singhal, Nayan
    - Egebo, Nick
    - Usunier, Nicolas
    - Mehta, Nikhil
    - Laptev, Nikolay Pavlovich
    - Dong, Ning
    - Cheng, Norman
    - Chernoguz, Oleg
    - Hart, Olivia
    - Salpekar, Omkar
    - Kalinli, Ozlem
    - Kent, Parkin
    - Parekh, Parth
    - Saab, Paul
    - Balaji, Pavan
    - Rittner, Pedro
    - Bontrager, Philip
    - Roux, Pierre
    - Dollar, Piotr
    - Zvyagina, Polina
    - Ratanchandani, Prashant
    - Yuvraj, Pritish
    - Liang, Qian
    - Alao, Rachad
    - Rodriguez, Rachel
    - Ayub, Rafi
    - Murthy, Raghotham
    - Nayani, Raghu
    - Mitra, Rahul
    - Parthasarathy, Rangaprabhu
    - Li, Raymond
    - Hogan, Rebekkah
    - Battey, Robin
    - Wang, Rocky
    - Howes, Russ
    - Rinott, Ruty
    - Mehta, Sachin
    - Siby, Sachin
    - Bondu, Sai Jayesh
    - Datta, Samyak
    - Chugh, Sara
    - Hunt, Sara
    - Dhillon, Sargun
    - Sidorov, Sasha
    - Pan, Satadru
    - Mahajan, Saurabh
    - Verma, Saurabh
    - Yamamoto, Seiji
    - Ramaswamy, Sharadh
    - Lindsay, Shaun
    - Lindsay, Shaun
    - Feng, Sheng
    - Lin, Shenghao
    - Zha, Shengxin Cindy
    - Patil, Shishir
    - Shankar, Shiva
    - Zhang, Shuqiang
    - Zhang, Shuqiang
    - Wang, Sinong
    - Agarwal, Sneha
    - Sajuyigbe, Soji
    - Chintala, Soumith
    - Max, Stephanie
    - Chen, Stephen
    - Kehoe, Steve
    - Satterfield, Steve
    - Govindaprasad, Sudarshan
    - Gupta, Sumit
    - Deng, Summer
    - Cho, Sungmin
    - Virk, Sunny
    - Subramanian, Suraj
    - Choudhury, Sy
    - Goldman, Sydney
    - Remez, Tal
    - Glaser, Tamar
    - Best, Tamara
    - Koehler, Thilo
    - Robinson, Thomas
    - Li, Tianhe
    - Zhang, Tianjun
    - Matthews, Tim
    - Chou, Timothy
    - Shaked, Tzook
    - Vontimitta, Varun
    - Ajayi, Victoria
    - Montanez, Victoria
    - Mohan, Vijai
    - Kumar, Vinay Satish
    - Mangla, Vishal
    - Ionescu, Vlad
    - Poenaru, Vlad
    - Mihailescu, Vlad Tiberiu
    - Ivanov, Vladimir
    - Li, Wei
    - Wang, Wenchen
    - Jiang, Wenwen
    - Bouaziz, Wes
    - Constable, Will
    - Tang, Xiaocheng
    - Wu, Xiaojian
    - Wang, Xiaolan
    - Wu, Xilun
    - Gao, Xinbo
    - Kleinman, Yaniv
    - Chen, Yanjun
    - Hu, Ye
    - Jia, Ye
    - Qi, Ye
    - Li, Yenda
    - Zhang, Yilin
    - Zhang, Ying
    - Adi, Yossi
    - Nam, Youngjin
    - Yu
    - Wang
    - Zhao, Yu
    - Hao, Yuchen
    - Qian, Yundi
    - Li, Yunlu
    - He, Yuzi
    - Rait, Zach
    - DeVito, Zachary
    - Rosnbrick, Zef
    - Wen, Zhaoduo
    - Yang, Zhenyu
    - Zhao, Zhiwei
    - Ma, Zhiyu
  date: 2024
  serial-number:
    doi: "10.48550/arXiv.2407.21783"

qwen:
  type: report
  title: Qwen Technical Report
  author:
    - Bai, Jinze
    - Bai, Shuai
    - Chu, Yunfei
    - Cui, Zeyu
    - Dang, Kai
    - Deng, Xiaodong
    - Fan, Yang
    - Ge, Wenbin
    - Han, Yu
    - Huang, Fei
    - Hui, Binyuan
    - Ji, Luo
    - Li, Mei
    - Lin, Junyang
    - Lin, Runji
    - Liu, Dayiheng
    - Liu, Gao
    - Lu, Chengqiang
    - Lu, Keming
    - Ma, Jianxin
    - Men, Rui
    - Ren, Xingzhang
    - Ren, Xuancheng
    - Tan, Chuanqi
    - Tan, Sinan
    - Tu, Jianhong
    - Wang, Peng
    - Wang, Shijie
    - Wang, Wei
    - Wu, Shengguang
    - Xu, Benfeng
    - Xu, Jin
    - Yang, An
    - Yang, Hao
    - Yang, Jian
    - Yang, Shusheng
    - Yao, Yang
    - Yu, Bowen
    - Yuan, Hongyi
    - Yuan, Zheng
    - Zhang, Jianwei
    - Zhang, Xingxuan
    - Zhang, Yichang
    - Zhang, Zhenru
    - Zhou, Chang
    - Zhou, Jingren
    - Zhou, Xiaohuan
    - Zhu, Tianhang
  date: 2023
  serial-number:
    doi: "10.48550/arXiv.2309.16609"

squad1:
  type: article
  title: "{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text"
  author:
    - Rajpurkar, Pranav
    - Zhang, Jian
    - Lopyrev, Konstantin
    - Liang, Percy
  date: 2016-11
  editor:
    - Su, Jian
    - Duh, Kevin
    - Carreras, Xavier
  page-range: 2383-2392
  url: https://aclanthology.org/D16-1264
  serial-number:
    doi: 10.18653/v1/D16-1264
  parent:
    type: proceedings
    title: Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing
    publisher: Association for Computational Linguistics
    location: Austin, Texas

squad2:
  type: article
  title: "Know What You Don{'}t Know: Unanswerable Questions for {SQ}u{AD}"
  author:
    - Rajpurkar, Pranav
    - Jia, Robin
    - Liang, Percy
  date: 2018-07
  editor:
    - Gurevych, Iryna
    - Miyao, Yusuke
  page-range: 784-789
  url: https://aclanthology.org/P18-2124
  serial-number:
    doi: 10.18653/v1/P18-2124
  abstract: "Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuADRUn, a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuADRUn, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuADRUn is a challenging natural language understanding task for existing models: a strong neural system that gets 86{%} F1 on SQuAD achieves only 66{%} F1 on SQuADRUn. We release SQuADRUn to the community as the successor to SQuAD."
  parent:
    type: proceedings
    title: "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"
    publisher: Association for Computational Linguistics
    location: Melbourne, Australia

multitask:
  type: article
  title: Multitask Learning
  author: Caruana, Rich
  date: 1997
  page-range: 41-75
  url: https://doi.org/10.1023/A:1007379606734
  serial-number:
    doi: 10.1023/A:1007379606734
    issn: 1573-0565
  abstract: Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals, and presents new results for MTL with k-nearest neighbor and kernel regression. In this paper we demonstrate multitask learning in three domains. We explain how multitask learning works, and show that there are many opportunities for multitask learning in real domains. We present an algorithm and results for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Because multitask learning works, can be applied to many different kinds of domains, and can be used with different learning algorithms, we conjecture there will be many opportunities for its use on real-world problems.
  parent:
    type: periodical
    title: Machine Learning
    issue: 1
    volume: 28

multitask-bert1:
  type: article
  author: Sanjaye Elayattu
  title: "Multi-task Fine-tuning with BERT"
  url: https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1234/final-reports/final-report-169425270.pdf

multitask-bert2:
  type: article
  title: "Multitasking with BERT"
  author:
    - Jiacheng Hu
    - Jack Hung
  url: https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1234/final-reports/final-report-169989122.pdf

hierarchical:
  type: article
  title: A survey of hierarchical classification across different application domains
  author:
    - Silla, Carlos N.
    - Freitas, Alex A.
  date: 2011
  page-range: 31-72
  serial-number:
    doi: 10.1007/s10618-010-0175-9
    issn: 1573-756X
  abstract: In this survey we discuss the task of hierarchical classification. The literature about this field is scattered across very different application domains and for that reason research in one domain is often done unaware of methods developed in other domains. We define what is the task of hierarchical classification and discuss why some related tasks should not be considered hierarchical classification. We also present a new perspective about some existing hierarchical classification approaches, and based on that perspective we propose a new unifying framework to classify the existing approaches. We also present a review of empirical comparisons of the existing methods reported in the literature as well as a conceptual comparison of those methods at a high level of abstraction, discussing their advantages and disadvantages.
  parent:
    type: periodical
    title: Data Mining and Knowledge Discovery
    issue: 1
    volume: 22

Opitz_2024:
  type: article
  title: A Closer Look at Classification Evaluation Metrics and a Critical Reflection of Common Evaluation Practice
  author: Opitz, Juri
  date: 2024
  page-range: 820-836
  url: http://dx.doi.org/10.1162/tacl_a_00675
  serial-number:
    doi: 10.1162/tacl_a_00675
    issn: 2307-387X
  parent:
    type: periodical
    title: Transactions of the Association for Computational Linguistics
    publisher: MIT Press
    volume: 12

muc-history:
  type: article
  title: "Message Understanding Conference-6: a brief history"
  author:
    - Grishman, Ralph
    - Sundheim, Beth
  date: 1996
  page-range: 466-471
  url: https://doi.org/10.3115/992628.992709
  serial-number:
    doi: 10.3115/992628.992709
  abstract: We have recently completed the sixth in a series of "Message Understanding Conferences" which are designed to promote and evaluate research in information extraction. MUC-6 introduced several innovations over prior MUCs, most notably in the range of different tasks for which evaluations were conducted. We describe some of the motivations for the new format and briefly discuss some of the results of the evaluations.
  parent:
    type: proceedings
    title: Proceedings of the 16th Conference on Computational Linguistics - Volume 1
    publisher: Association for Computational Linguistics
    location: Copenhagen, Denmark
    parent:
      type: proceedings
      title: COLING '96

munnangi2024briefhistorynamedentity:
  type: article
  title: A Brief History of Named Entity Recognition
  author: Munnangi, Monica
  date: 2024
  serial-number:
    doi: 10.48550/arXiv.2411.05057

crf-base:
  type: article
  title: "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data"
  author:
    - Lafferty, John D.
    - McCallum, Andrew
    - Pereira, Fernando C. N.
  date: 2001
  page-range: 282-289
  serial-number:
    doi: 10.5555/645530.655813
    isbn: "1558607781"
  parent:
    type: proceedings
    title: Proceedings of the Eighteenth International Conference on Machine Learning
    publisher: Morgan Kaufmann Publishers Inc.
    location: San Francisco, CA, USA
    parent:
      type: proceedings
      title: ICML '01

svm:
  type: article
  title: Support-vector networks
  author:
    - Cortes, Corinna
    - Vapnik, Vladimir
  date: 1995-09-01
  page-range: 273-297
  url: https://doi.org/10.1007/BF00994018
  serial-number:
    doi: 10.1007/BF00994018
    issn: 1573-0565
  abstract: "Thesupport-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data."
  parent:
    type: periodical
    title: Machine Learning
    issue: 3
    volume: 20

Polignano2021818:
  type: article
  title: Comparing transformer-based NER approaches for analysing textual medical diagnoses
  author:
    - Polignano, Marco
    - name: Gemmis
      given-name: Marco
      prefix: de
    - Semeraro, Giovanni
  date: 2021
  editor:
    - G., Faggioli
    - name: Padova
      given-name: Via Gradenigo 6/b, Padova
      prefix: University of
      suffix: Department of Information Engineering
    - N., Ferro
    - name: Padova
      given-name: Via Gradenigo 6/b, Padova
      prefix: University of
      suffix: Department of Information Engineering
    - A., Joly
    - name: Montpellier
      given-name: Inria ZENITH, 161 Rue Ada, Montpellier Cedex 5
      prefix: University of
      suffix: LIRMM
    - M., Maistro
    - name: Copenhagen
      given-name: Universitetsparken 1, Copenhagen
      prefix: University of
      suffix: Department of Computer Science
    - F., Piroi
    - name: Technology (TU)
      given-name: Favoritenstrasse 9-11/188, Vienna
      prefix: Vienna University of
      suffix: Institute of Information Systems Engineering
  page-range: 818-833
  url: https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113468144&partnerID=40&md5=415546a367cea8c30e30881072935304
  serial-number:
    issn: "16130073"
  abstract: The automated analysis of medical documents has grown in research interest in recent years as a consequence of the social relevance of the thematic and the difficulties often encountered with short and very specific documents. In particular, this fervent area of research has stimulated the development of several techniques of automatic document classification, question answering, and name entity recognition (NER). Nevertheless, many open issues must be addressed to obtain results that are satisfactory for a field in which the effectiveness of predictions is a fundamental factor in order not to make mistakes that could compromise people's lives. To this end, we focused on the name entity recognition task from medical documents and, in this work, we will discuss the results we obtained by our hybrid approach. In order to take advantage of the most relevant findings in the field of natural language processing, we decided to focus on deep neural network models. We compared several configurations of our model by varying the transformer architecture, such as BERT, RoBERTa and ELECTRA, until we obtained a configuration that we considered the best for our goals. The most promising model was used to participate in the SpRadIE task of the annual CLEF (Conference and Labs of the Evaluation Forum). The obtained results are encouraging and can be of reference for future studies on the topic. © 2021 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).
  parent:
    type: proceedings
    publisher: CEUR-WS
    volume: 2936

Schank1975ScriptsPA:
  type: article
  title: Scripts, plans, and knowledge
  author:
    - Schank, Roger C.
    - Abelson, Robert P.
  date: 1975
  url: https://api.semanticscholar.org/CorpusID:264593435
  parent:
    type: proceedings
    title: International Joint Conference on Artificial Intelligence

slot2:
  type: article
  title: A statistical approach to machine translation
  author:
    - Brown, Peter F.
    - Cocke, John
    - Pietra, Stephen A. Della
    - Pietra, Vincent J. Della
    - Jelinek, Fredrick
    - Lafferty, John D.
    - Mercer, Robert L.
    - Roossin, Paul S.
  date: 1990-06
  page-range: 79-85
  serial-number:
    issn: 0891-2017
    doi: 10.5555/92858.92860
  abstract: In this paper, we present a statistical approach to machine translation. We describe the application of our approach to translation from French to English and give preliminary results.
  parent:
    type: periodical
    title: Comput. Linguist.
    publisher: MIT Press
    location: Cambridge, MA, USA
    issue: 2
    volume: 16

hmm:
  type: article
  title: Maximum Entropy Markov Models for Information Extraction and Segmentation
  author:
    - McCallum, Andrew
    - Freitag, Dayne
    - Pereira, Fernando C. N.
  date: 2000
  page-range: 591-598
  serial-number:
    isbn: "1558607072"
    doi: 10.5555/645529.658277
  parent:
    type: proceedings
    title: Proceedings of the Seventeenth International Conference on Machine Learning
    publisher: Morgan Kaufmann Publishers Inc.
    location: San Francisco, CA, USA
    parent:
      type: proceedings
      title: ICML '00

ner-slot-joint:
  type: article
  title: Exploring Named Entity Recognition As an Auxiliary Task for Slot Filling in Conversational Language Understanding
  author:
    - Louvan, Samuel
    - Magnini, Bernardo
  date: 2018-10
  editor:
    - Chuklin, Aleksandr
    - Dalton, Jeff
    - Kiseleva, Julia
    - Borisov, Alexey
    - Burtsev, Mikhail
  page-range: 74-80
  url: https://aclanthology.org/W18-5711/
  serial-number:
    doi: 10.18653/v1/W18-5711
  abstract: Slot filling is a crucial task in the Natural Language Understanding (NLU) component of a dialogue system. Most approaches for this task rely solely on the domain-specific datasets for training. We propose a joint model of slot filling and Named Entity Recognition (NER) in a multi-task learning (MTL) setup. Our experiments on three slot filling datasets show that using NER as an auxiliary task improves slot filling performance and achieve competitive performance compared with state-of-the-art. In particular, NER is effective when supervised at the lower layer of the model. For low-resource scenarios, we found that MTL is effective for one dataset.
  parent:
    type: proceedings
    title: "Proceedings of the 2018 {EMNLP} Workshop {SCAI}: The 2nd International Workshop on Search-Oriented Conversational {AI}"
    publisher: Association for Computational Linguistics
    location: Brussels, Belgium

scikit-learn:
  type: article
  title: "Scikit-learn: Machine Learning in Python"
  author:
    - Pedregosa, Fabian
    - Varoquaux, Gaël
    - Gramfort, Alexandre
    - Michel, Vincent
    - Thirion, Bertrand
    - Grisel, Olivier
    - Blondel, Mathieu
    - Müller, Andreas
    - Nothman, Joel
    - Louppe, Gilles
    - Prettenhofer, Peter
    - Weiss, Ron
    - Dubourg, Vincent
    - Vanderplas, Jake
    - Passos, Alexandre
    - Cournapeau, David
    - Brucher, Matthieu
    - Perrot, Matthieu
    - Duchesnay, Édouard
  date: 2018
  serial-number:
    doi: 10.48550/arXiv.1201.0490

honnibal_spacy_2015:
  type: misc
  title: Introducing spaCy
  author:
    - Honnibal, Matthew
    - Montani, Ines
  date: 2015
  url: https://explosion.ai/blog/introducing-spacy

spacy:
  type: misc
  title: "spaCy: Industrial-strength Natural Language Processing in Python"
  author:
    - Honnibal, Matthew
    - Montani, Ines
  date: 2020
  url: https://spacy.io/

gplv3:
  type: misc
  title: GNU General Public License
  date: 2007-06-29
  organization: Free Software Foundation
  url: http://www.gnu.org/licenses/gpl.html
  serial-number:
    version: "3"

fsfs:
  type: book
  title: "Free software free society: selected essays of Richard M. Stallman"
  author: Stallman, Richard
  date: 2015
  publisher: Free Software Foundation
  location: Boston, Mass.
  edition: 3rd
  url: https://www.gnu.org/doc/fsfs3-hardcover.pdf
  serial-number:
    isbn: 978-0-9831592-5-4

doccano:
  type: misc
  title: "{doccano}: Text Annotation Tool for Human"
  author:
    - Nakayama, Hiroki
    - Kubo, Takahiro
    - Kamura, Junya
    - Taniguchi, Yasufumi
    - Liang, Xu
  date: 2018
  url: https://github.com/doccano/doccano

jmespath:
  type: web
  title: "JMESPath is a query language for JSON"
  author:
    - James Saryerwinnie
  url: https://jmespath.org/

jsonpath:
  type: blog
  title: "JSONPath - XPath for JSON"
  author:
    - Stefan Goessner
  date: 2007
  url: https://goessner.net/articles/JsonPath/

sparql:
  type: report
  title: "SPARQL Query Language for RDF"
  organization: W3C
  url: https://www.w3.org/TR/sparql11-query/

graphviz:
  type: reference
  title: "Graphviz - Graph Visualization Software"
  organization: Graphviz
  url: https://graphviz.org/

handlebars:
  type: reference
  title: "Handlebars - Minimal Templating on Steroids"
  organization: Handlebars
  url: https://handlebarsjs.com/

fastapi:
  type: reference
  title: "FastAPI - Fast (high-performance) API framework for Python"
  organization: FastAPI
  url: https://fastapi.tiangolo.com/

owasp-injection:
  type: report
  title: SQL Injection Prevention Cheat Sheet
  url: https://cheatsheetseries.owasp.org/cheatsheets/SQL_Injection_Prevention_Cheat_Sheet.html

rdf-syntax-grammar:
  type: report
  title: "RDF 1.1 XML Syntax"
  author: W3C
  date: 2014-02-25
  organization: "World Wide Web Consortium"
  abstract: This document defines an XML syntax for RDF called RDF/XML in terms of Namespaces in XML, the XML Information Set and XML Base.
  url: https://www.w3.org/TR/rdf-syntax-grammar/

rdf-turtle:
  type: report
  title: "RDF 1.1 Turtle"
  author: W3C
  date: 2025-02-20
  organization: "World Wide Web Consortium"
  url: https://w3c.github.io/rdf-turtle/spec/

dbpedia:
  type: web
  title: "DBpedia"
  url: https://wiki.dbpedia.org/

wikidata:
  type: web
  title: "Wikidata"
  url: https://www.wikidata.org/

Rajaraman_Ullman_2011:
  type: chapter
  title: Data Mining
  author:
    - Rajaraman, Anand
    - Ullman, Jeffrey David
  date: 2011
  page-range: 1-17
  parent:
    type: book
    title: Mining of Massive Datasets
    publisher: Cambridge University Press

okapi-bm25:
  type: article
  title: Okapi at TREC-3.
  author:
    - Robertson, Stephen
    - Walker, Steve
    - Jones, Susan
    - Hancock-Beaulieu, Micheline
    - Gatford, Mike
  date: 1994
  parent:
    type: proceedings

sentence-bert:
  type: misc
  title: "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"
  author:
    - Reimers, Nils
    - Gurevych, Iryna
  date: 2019
  url: https://arxiv.org/abs/1908.10084
  serial-number:
    arxiv: "1908.10084"

wang-etal-2021-retrieval:
  type: article
  title: Retrieval, Re-ranking and Multi-task Learning for Knowledge-Base Question Answering
  author:
    - Wang, Zhiguo
    - Ng, Patrick
    - Nallapati, Ramesh
    - Xiang, Bing
  date: 2021-04
  editor:
    - Merlo, Paola
    - Tiedemann, Jorg
    - Tsarfaty, Reut
  page-range: 347-357
  url: https://aclanthology.org/2021.eacl-main.26/
  serial-number:
    doi: 10.18653/v1/2021.eacl-main.26
  abstract: 'Question answering over knowledge bases (KBQA) usually involves three sub-tasks, namely topic entity detection, entity linking and relation detection. Due to the large number of entities and relations inside knowledge bases (KB), previous work usually utilized sophisticated rules to narrow down the search space and managed only a subset of KBs in memory. In this work, we leverage a \\textit\{retrieve-and-rerank\} framework to access KBs via traditional information retrieval (IR) method, and re-rank retrieved candidates with more powerful neural networks such as the pre-trained BERT model. Considering the fact that directly assigning a different BERT model for each sub-task may incur prohibitive costs, we propose to share a BERT encoder across all three sub-tasks and define task-specific layers on top of the shared layer. The unified model is then trained under a multi-task learning framework. Experiments show that: (1) Our IR-based retrieval method is able to collect high-quality candidates efficiently, thus enables our method adapt to large-scale KBs easily; (2) the BERT model improves the accuracy across all three sub-tasks; and (3) benefiting from multi-task learning, the unified model obtains further improvements with only 1/3 of the original parameters. Our final model achieves competitive results on the SimpleQuestions dataset and superior performance on the FreebaseQA dataset.'
  parent:
    type: proceedings
    title: "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume"
    publisher: Association for Computational Linguistics
    location: Online

kasner-dusek:
  type: article
  title: "Beyond Traditional Benchmarks: Analyzing Behaviors of Open {LLM}s on Data-to-Text Generation"
  author:
    - Kasner, Zdeněk
    - Dušek, Ondřej
  date: 2024
  editor:
    - Ku, Lun-Wei
    - Martins, Andre
    - Srikumar, Vivek
  page-range: 12045-12072
  url: https://aclanthology.org/2024.acl-long.651
  parent:
    type: proceedings
    title: "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"
    publisher: Association for Computational Linguistics
    location: Bangkok, Thailand

llm-zeroshot:
  type: article
  title: Using Large Language Models for Zero-Shot Natural Language Generation from Knowledge Graphs
  author:
    - Axelsson, Agnes
    - Skantze, Gabriel
  date: 2023
  url: https://arxiv.org/abs/2307.07312
  serial-number:
    arxiv: "2307.07312"

yuan-faerber-graph2text:
  type: article
  title: Evaluating Generative Models for Graph-to-Text Generation
  author:
    - Yuan, Shuzhou
    - Faerber, Michael
  date: 2023-09
  editor:
    - Mitkov, Ruslan
    - Angelova, Galia
  page-range: 1256-1264
  url: https://aclanthology.org/2023.ranlp-1.133/
  abstract: Large language models (LLMs) have been widely employed for graph-to-text generation tasks. However, the process of finetuning LLMs requires significant training resources and annotation work. In this paper, we explore the capability of generative models to generate descriptive text from graph data in a zero-shot setting. Specifically, we evaluate GPT-3 and ChatGPT on two graph-to-text datasets and compare their performance with that of finetuned LLM models such as T5 and BART. Our results demonstrate that generative models are capable of generating fluent and coherent text, achieving BLEU scores of 10.57 and 11.08 for the AGENDA and WebNLG datasets, respectively. However, our error analysis reveals that generative models still struggle with understanding the semantic relations between entities, and they also tend to generate text with hallucinations or irrelevant information. As a part of error analysis, we utilize BERT to detect machine-generated text and achieve high macro-F1 scores. We have made the text generated by generative models publicly available.
  parent:
    type: proceedings
    title: Proceedings of the 14th International Conference on Recent Advances in Natural Language Processing
    publisher: INCOMA Ltd., Shoumen, Bulgaria
    location: Varna, Bulgaria

paper-dataset-nova:
  type: article
  title: "Educational Dialogue Systems for Visually Impaired Students: Introducing a Task-Oriented User-Agent Corpus"
  author:
    - Nuovo, Elisa Di
    - Sanguinetti, Manuela
    - Balestrucci, Pier Felice
    - Anselma, Luca
    - Bernareggi, Cristian
    - Mazzei, Alessandro
  date: 2024-05-25
  editor:
    - Calzolari, Nicoletta
    - Kan, Min-Yen
    - Hoste, Veronique
    - Lenci, Alessandro
    - Sakti, Sakriani
    - Xue, Nianwen
  page-range: 5507-5519
  url: https://iris.unito.it/handle/2318/1977310
  serial-number:
    isbn: 978-2-493814-10-4
  parent:
    type: proceedings
    title: Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)
    publisher: ELRA, ICCL
    location: Luxembourg

dataset-nova:
  type: repository
  title: NoVAGraphS FSA User-Agent Corpus
  author:
    - Di Nuovo, Elisa
    - Sanguinetti, Manuela
    - Mazzei, Alessandro
    - Anselma, Luca
    - Bernareggi, Cristian
    - Balestrucci, Pier Felice
  date: 2024-03
  publisher: Zenodo
  url: https://doi.org/10.5281/zenodo.10822733
  serial-number:
    doi: 10.5281/zenodo.10822733
    version: "1.0"

mazzei-novagraphs:
  type: article
  title: "NoVAGraphS: Towards an Accessible Educational-Oriented Dialogue System"
  author:
    - Michael Oliverio
    - Margherita Piroi
    - Daniele De Giorgi
    - Pier Felice Balestrucci
    - Carola Manolino
    - Alessandro Mazzei
    - Luca Anselma
    - Cristian Bernareggi
    - Marina Serio
    - Cristina Sabena
    - Tiziana Armano
    - Sandro Coriasco
    - Anna Capietto
  date: 2024
  editor:
    - Taibi, Davide
    - Schicchi, Daniele
    - Temperini, Marco
    - Limongelli, Carla
    - Casalino, Gabriella
  page-range: 1-12
  url: https://iris.unito.it/handle/2318/2041270
  parent:
    type: proceedings
    title: Proceedings of the Second International Workshop on Artificial Intelligent Systems in Education co-located with 23rd International Conference of the Italian Association for Artificial Intelligence (AIxIA 2024)
    publisher: CEUR-WS
    location: Aachen, Germany
    volume: 3879
    parent:
      type: proceedings
      title: CEUR Workshop Proceedings

nova-spoken:
  type: article
  title: Building a Spoken Dialogue System for Supporting Blind People in Accessing Mathematical Expressions
  author:
    - Balestrucci, Pier Felice
    - Anselma, Luca
    - Bernareggi, Cristian
    - Mazzei, Alessandro
  date: 2023
  editor:
    - Boschetti, Federico
    - Lebani, Gianluca E.
    - Magnini, Bernardo
    - Novielli, Nicole
  page-range: 1-8
  url: https://iris.unito.it/handle/2318/1950755
  parent:
    type: proceedings
    title: Proceedings of the 9th Italian Conference on Computational Linguistics (CLiC-it 2023)
    publisher: CEUR-WS
    location: Aachen, Germany
    volume: 3596
    parent:
      type: proceedings
      title: CEUR Workshop Proceedings

nova-edu:
  type: article
  title: An Educational Dialogue System for Visually Impaired People
  author:
    - Balestrucci, Pier Felice
    - Nuovo, Elisa Di
    - Sanguinetti, Manuela
    - Anselma, Luca
    - Bernareggi, Cristian
    - Mazzei, Alessandro
  date: 2024
  page-range: 150502-150519
  url: https://ieeexplore.ieee.org/document/10716367
  serial-number:
    doi: 10.1109/access.2024.3479883
  parent:
    type: periodical
    title: IEEE Access
    volume: 12

kasner2024factgenie:
  type: article
  title: "factgenie: A Framework for Span-based Evaluation of Generated Texts"
  author:
    - Kasner, Zdeněk
    - Platek, Ondrej
    - Schmidtova, Patricia
    - Balloccu, Simone
    - Dusek, Ondrej
  date: 2024
  editor:
    - Mahamood, Saad
    - Minh, Nguyen Le
    - Ippolito, Daphne
  page-range: 13–15
  url: https://aclanthology.org/2024.inlg-demos.5
  parent:
    type: proceedings
    title: "Proceedings of the 17th International Natural Language Generation Conference: System Demonstrations"
    publisher: Association for Computational Linguistics
    location: Tokyo, Japan

factgenie:
  type: repository
  title: Factgenie
  url: https://github.com/ufal/factgenie

gpt-good-eval-wang:
  type: article
  title: Is ChatGPT a Good NLG Evaluator? A Preliminary Study
  author:
    - Wang, Jiaan
    - Liang, Yunlong
    - Meng, Fandong
    - Sun, Zengkui
    - Shi, Haoxiang
    - Li, Zhixu
    - Xu, Jinan
    - Qu, Jianfeng
    - Zhou, Jie
  date: 2023
  serial-number:
    doi: 10.48550/arXiv.2303.04048
    arxiv: "2303.04048"

sottana-etal-2023-evaluation:
  type: article
  title: "Evaluation Metrics in the Era of GPT-4: Reliably Evaluating Large Language Models on Sequence to Sequence Tasks"
  author:
    - Sottana, Andrea
    - Liang, Bin
    - Zou, Kai
    - Yuan, Zheng
  date: 2023-12
  editor:
    - Bouamor, Houda
    - Pino, Juan
    - Bali, Kalika
  page-range: 8776–8788
  url: https://aclanthology.org/2023.emnlp-main.543/
  serial-number:
    doi: 10.18653/v1/2023.emnlp-main.543
  abstract: "Large Language Models (LLMs) evaluation is a patchy and inconsistent landscape, and it is becoming clear that the quality of automatic evaluation metrics is not keeping up with the pace of development of generative models. We aim to improve the understanding of current models' performance by providing a preliminary and hybrid evaluation on a range of open and closed-source generative LLMs on three NLP benchmarks: text summarisation, text simplification and grammatical error correction (GEC), using both automatic and human evaluation. We also explore the potential of the recently released GPT-4 to act as an evaluator. We find that ChatGPT consistently outperforms many other popular models according to human reviewers on the majority of metrics, while scoring much more poorly when using classic automatic evaluation metrics. We also find that human reviewers rate the gold reference as much worse than the best models' outputs, indicating the poor quality of many popular benchmarks. Finally, we find that GPT-4 is capable of ranking models' outputs in a way which aligns reasonably closely to human judgement despite task-specific variations, with a lower alignment in the GEC task."
  parent:
    type: proceedings
    title: Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing
    publisher: Association for Computational Linguistics
    location: Singapore

kocmi-federmann-2023-gemba:
  type: article
  title: "GEMBA-MQM: Detecting Translation Quality Error Spans with {GPT}-4"
  author:
    - Kocmi, Tom
    - Federmann, Christian
  date: 2023-12
  editor:
    - Koehn, Philipp
    - Haddow, Barry
    - Kocmi, Tom
    - Monz, Christof
  page-range: 768–775
  url: https://aclanthology.org/2023.wmt-1.64/
  serial-number:
    doi: 10.18653/v1/2023.wmt-1.64
  abstract: This paper introduces GEMBA-MQM, a GPT-based evaluation metric designed to detect translation quality errors, specifically for the quality estimation setting without the need for human reference translations. Based on the power of large language models (LLM), GEMBA-MQM employs a fixed three-shot prompting technique, querying the GPT-4 model to mark error quality spans. Compared to previous works, our method has language-agnostic prompts, thus avoiding the need for manual prompt preparation for new languages. While preliminary results indicate that GEMBA-MQM achieves state-of-the-art accuracy for system ranking, we advise caution when using it in academic works to demonstrate improvements over other methods due to its dependence on the proprietary, black-box GPT model.
  parent:
    type: proceedings
    title: Proceedings of the Eighth Conference on Machine Translation
    publisher: Association for Computational Linguistics
    location: Singapore

chain-of-thought:
  type: article
  title: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models
  author:
    - Wei, Jason
    - Wang, Xuezhi
    - Schuurmans, Dale
    - Bosma, Maarten
    - Ichter, Brian
    - Xia, Fei
    - Chi, Ed
    - Le, Quoc
    - Zhou, Denny
  date: 2023
  serial-number:
    doi: 10.48550/arXiv.2201.11903
    arxiv: "2201.11903"
  parent:
    type: periodical

open-source-llm:
  type: article
  url: https://hackernoon.com/is-that-llm-actually-open-source-we-need-to-talk-open-washing-in-ai-governance
  title: Is that LLM Actually "Open Source"? We Need to Talk About Open-Washing in AI Governance
  author: Sal Kimmich
  date: 2024-09-8

open-weights-foundational:
  type: article
  url: https://www.ftc.gov/policy/advocacy-research/tech-at-ftc/2024/07/open-weights-foundation-models
  title: On Open-Weights Foundation Models

hassio:
  type: web
  title: "Home Assistant"
  url: https://www.home-assistant.io/

idiosyncrasieslargelanguagemodels:
  type: article
  title: Idiosyncrasies in Large Language Models
  author:
    - Sun, Mingjie
    - Yin, Yida
    - Xu, Zhiqiu
    - Kolter, J. Zico
    - Liu, Zhuang
  date: 2025
  url: https://arxiv.org/abs/2502.12150
  serial-number:
    arxiv: "2502.12150"

abacha2025medecbenchmarkmedicalerror:
  type: article
  title: "MEDEC: A Benchmark for Medical Error Detection and Correction in Clinical Notes"
  author:
    - Abacha, Asma Ben
    - Yim, Wen-wai
    - Fu, Yujuan
    - Sun, Zhaoyi
    - Yetisgen, Meliha
    - Xia, Fei
    - Lin, Thomas
  date: 2025
  url: https://arxiv.org/abs/2412.19260
  serial-number:
    doi: 10.48550/arXiv.2412.19260
    arxiv: "2412.19260"

rig-performance:
  type: article
  title: Improving Your Model Ranking on Chatbot Arena by Vote Rigging
  author:
    - Min, Rui
    - Pang, Tianyu
    - Du, Chao
    - Liu, Qian
    - Cheng, Minhao
    - Lin, Min
  date: 2025
  serial-number:
    doi: 10.48550/arXiv.2501.17858
    arxiv: "2501.17858"

bycloud-rigging:
  type: video
  title: Cheating LLM Benchmarks Is Easier Than You Think…
  url: https://www.youtube.com/watch?v=IbBEbjeVWgI
  author: bycloud
  date: 2025-03-10

aiml-high:
  type: repository
  author: gleuch
  url: https://github.com/gleuch/aiml-high

python-aiml:
  type: repository
  author: paulovn
  url: https://github.com/paulovn/python-aiml

strangmann2024transferlearningfinetuninglarge:
  type: article
  title: Transfer Learning for Finetuning Large Language Models
  author:
    - Strangmann, Tobias
    - Purucker, Lennart
    - Franke, Jörg K. H.
    - Rapant, Ivo
    - Ferreira, Fabio
    - Hutter, Frank
  date: 2024
  serial-number:
    doi: 10.48550/arXiv.2411.01195
    arxiv: "2411.01195"

jetbrains-python:
  type: report
  title: Python Developers Survey 2021 Results
  url: https://lp.jetbrains.com/python-developers-survey-2021/
  date: 2021

webpage-wait-time:
  type: article
  title: 'A Study on Tolerable Waiting Time: How Long Are Web Users Willing to Wait?'
  author: Nah, Fiona
  date: 2003
  page-range: 285
  serial-number:
    doi: 10.1080/01449290410001669914
  parent:
    type: proceedings
    volume: 23

yaml:
  type: web
  title: "YAML: YAML Ain't Markup Language™"
  url: https://yaml.org/

cameronwolfe:
  type: post
  author: Cameron Wolfe
  date: 2024-03-04
  title: "Decoder-Only Transformers: The Workhorse of Generative LLMs"
  url: https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse

code-smells:
  type: web
  url: "https://c2.com/xp/CodeSmell.html"
  title: "Code Smells"

mixin:
  type: article
  title: Mixin-based inheritance
  author:
  - Bracha, Gilad
  - Cook, William
  date: 1990
  page-range: 303–311
  url: https://doi.org/10.1145/97945.97982
  serial-number:
    doi: 10.1145/97945.97982
    isbn: '0897914112'
  abstract: The diverse inheritance mechanisms provided by Smalltalk, Beta, and CLOS are interpreted as different uses of a single underlying construct. Smalltalk and Beta differ primarily in the direction of class hierarchy growth. These inheritance mechanisms are subsumed in a new inheritance model based on composition of mixins, or abstract subclasses. This form of inheritance can also encode a CLOS multiple-inheritance hierarchy, although changes to the encoded hierarchy that would violate encapsulation are difficult. Practical application of mixin-based inheritance is illustrated in a sketch of an extension to Modula-3.
  parent:
    type: proceedings
    title: Proceedings of the European Conference on Object-Oriented Programming on Object-Oriented Programming Systems, Languages, and Applications
    publisher: Association for Computing Machinery
    location: Ottawa, Canada
    parent:
      type: proceedings
      title: OOPSLA/ECOOP '90

composition-inheritance-java:
  type: article
  title: What Programmers Do with Inheritance in Java
  author:
  - Tempero, Ewan
  - Yang, Hong Yul
  - Noble, James
  date: 2013
  editor: Castagna, Giuseppe
  page-range: 577–601
  serial-number:
    isbn: 978-3-642-39038-8
  abstract: 'Inheritance is a distinguishing feature of object-oriented programming languages, but its application in practice remains poorly understood. Programmers employ inheritance for a number of different purposes: to provide subtyping, to reuse code, to allow subclasses to customise superclasses'' behaviour, or just to categorise objects. We present an empirical study of 93 open-source Java software systems consisting over over 200,000 classes and interfaces, supplemented by longitudinal analyses of 43 versions of two systems. Our analysis finds inheritance is used for two main reasons: to support subtyping and to permit what we call external code reuse. This is the first empirical study to indicate what programmers do with inheritance.'
  parent:
    type: proceedings
    title: ECOOP 2013 - Object-Oriented Programming
    publisher: Springer Berlin Heidelberg
    location: Berlin, Heidelberg

gof:
  type: book
  title: 'Design Patterns: Elements of Reusable Object-Oriented Software'
  author:
  - Gamma, Erich
  - Helm, Richard
  - Johnson, Ralph
  - Vlissides, John
  date: 1994
  publisher: Addison-Wesley Professional
  serial-number:
    isbn: 978-0201633610

asteval:
  type: repository
  url: https://github.com/lmfit/asteval/
  title: ASTEVAL
  author: lmfit