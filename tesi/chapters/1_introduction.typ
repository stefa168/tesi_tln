= Introduzione <cap1>
== Contesto
Quando si pensa alla preistoria, viene facile immaginare un mondo ostile e primordiale, in cui la lotta per la sopravvivenza era una sfida quotidiana. È poco diffusa la conoscenza che invece, proprio in quell'epoca lontanissima, sono nati i primi gesti di *mutuo soccorso* @disabilites-prehistory.\
Anche le tracce archeologiche più antiche, dai resti di individui guariti da gravi fratture alle sepolture rituali, raccontano di una comunità in cui chi non poteva più procacciarsi il cibo o difendersi dai pericoli non veniva abbandonato, ma curato e sostenuto dagli altri membri del gruppo.

Questa solidarietà “preistorica” non era una semplice parentesi, ma ha costituito un seme che, millennio dopo millennio, ha continuato a germogliare fino ai giorni nostri.\
Il fatto che i nostri antenati prestassero soccorso a chi era ferito o portatore di disabilità è un *segno distintivo dell'umanità* @prehistory-compassion. Con il passare del tempo, quella che poteva essere un'assistenza spontanea e basata sull'istinto si è trasformata in strutture sociali sempre più complesse, in cui il concetto di “prendersi cura” è diventato uno degli elementi chiave per la sopravvivenza della comunità. Da semplici gesti pratici si è passati a forme di medicina primitiva, fino a sviluppare ruoli specifici, come il “guaritore” o lo sciamano, che si prendevano cura dei membri più vulnerabili.

Volgendo lo sguardo al mondo d'oggi, ci troviamo in un'epoca in cui la tecnologia ha completamente rivoluzionato il modo di fornire sostegno a chi ne ha bisogno. Pensiamo, ad esempio, a quanto sono diventati comuni ausili come le _protesi robotiche_, i _sensori intelligenti_, gli *screenreader* per chi ha difficoltà visive o i software di *riconoscimento vocale* avanzati. Questa evoluzione tecnologica ha aperto la strada a una concezione dell'assistenza sempre più personalizzata e, soprattutto, in grado di ridurre drasticamente le barriere tra chi è affetto da una disabilità e il resto della società.

Quando parliamo di aiuto alle persone portatrici di disabilità, non ci limitiamo più a un'ottica di “cura” o “supporto”: il vero obiettivo, adesso, è l'*inclusione a tutto tondo*. Le innovazioni tecnologiche mirano a rendere accessibili aree fondamentali della vita quotidiana e professionale: dall'educazione alla partecipazione attiva nella comunità, fino all'accesso a contenuti culturali e artistici. È un salto di qualità che mette al centro il concetto di pari opportunità, in cui la tecnologia non è un semplice strumento di compensazione, ma un mezzo per esprimere pienamente il proprio potenziale, per assicurare *a ogni individuo la propria autonomia*.

== Motivazioni e obiettivi della tesi

In questo contesto di grande fermento e continua ricerca si inserisce il progetto NoVAGraphS @mazzei-novagraphs @nova-spoken. L'idea di base è tanto semplice quanto ambiziosa: sviluppare *sistemi di dialogo* che permettano alle persone con disabilità visive di comprendere e manipolare contenuti che di solito sono *fortemente visivi*, come diagrammi, grafi, schemi e mappe concettuali. In altri termini, si vuole “tradurre” questo genere di contenuti complessi in conversazioni strutturate, in modo che chi non può vedere possa comunque esplorare e navigare in modo autonomo all'interno essi.

Oggi i dispositivi di sintesi vocale (TTS, Text-To-Speech) e gli screenreader sono fondamentali per rendere accessibile il contenuto testuale alle persone con disabilità visive. Se un testo è codificato in maniera compatibile (per esempio, in formati come HTML accessibile o PDF taggati), questi sistemi possono “leggerlo” e trasformarlo in audio.

La situazione cambia drasticamente quando il contenuto è rappresentato da un'illustrazione o un'immagine: una mappa, un diagramma, un fumetto, un'infografica, o addirittura una semplice foto con informazioni incorporate. In questi casi, i tradizionali sistemi TTS, per quanto evoluti, non sono in grado di interpretare la struttura o il significato di un elemento puramente visivo e di tradurlo in un output che abbia senso per chi non vede. Si limiteranno invece a leggere una descrizione testuale associata all'immagine, che può essere generica o addirittura (molto spesso) assente @WebAIM2024.

È evidente, dunque, che l'accessibilità “di base” offerta da queste tecnologie deve essere arricchita da soluzioni che sappiano descrivere e narrare i contenuti grafici, permettendo non soltanto di “sentire” una descrizione generica dell'immagine, ma anche di esplorarla in modo approfondito. È proprio in questa direzione che si muove il progetto NoVAGraphS, attraverso l'idea di creare un dialogo interattivo in cui l'utente possa fare domande, focalizzare l'attenzione su certi elementi dell'immagine e chiedere chiarimenti, superando così il limite puramente testuale dei sistemi TTS tradizionali.

Nel progetto, largo spazio è dedicato allo sviluppo di interfacce testuali o vocali, dove un “assistente virtuale” dialoga con l'utente, fornendo descrizioni, chiarimenti e spiegazioni passo dopo passo. Si tratta di un ambito in cui convergono competenze di Intelligenza Artificiale, Elaborazione del Linguaggio Naturale (NLP) e design di interfacce accessibili. La sfida è proprio quella di realizzare una tecnologia che sappia interpretare sia le esigenze dell'utente, sia la complessità del contenuto grafico da descrivere.

Uno dei cardini di NoVAGraphS è la capacità di “smontare” un diagramma in tutte le sue componenti fondamentali (linee, nodi, connessioni, strutture gerarchiche) per poi renderle accessibili tramite un sistema di dialogo.

Quando si parla di “sistema di dialogo” (o "chatbot") ci si riferisce a un software progettato per interagire con le persone attraverso uno scambio di messaggi, solitamente in linguaggio naturale. A differenza di un semplice motore di ricerca, che recupera documenti contenenti determinate parole chiave, un chatbot è in grado di comprendere (fino a un certo punto) l'intento dell'utente e restituire una risposta mirata, che può essere testuale o, usando sistemi di TTS, vocale.

Grazie ai progressi nell'elaborazione del linguaggio naturale (NLP) e nell'intelligenza artificiale, i chatbot oggi sono in grado di sostenere conversazioni più o meno complesse, simulando la sensazione di dialogare con un assistente umano. Ciò li rende strumenti particolarmente interessanti in ambito assistivo, dove l'interazione discorsiva può semplificare l'accesso a informazioni e contenuti che, altrimenti, sarebbero difficili o addirittura impossibili da fruire per persone con disabilità visive.

In questo modo, se una persona cieca o ipovedente chiedesse “Quali sono gli elementi principali di questo diagramma?”, il sistema sarà in grado di rispondere elencandoli e, se necessario, spiegando come sono collegati fra loro. L'obiettivo non è fornire una descrizione generica, ma consentire una vera e propria interazione mirata alla comprensione, così come avviene quando si osserva un'immagine e ci si concentra via via su parti diverse.

== Struttura del documento

All'interno dello scenario presentato, la mia tesi si propone di descrivere il percorso di analisi, progettazione e realizzazione di un sistema utile agli scopi del progetto NoVAGraphS. In particolare, illustrerò come abbiamo affrontato la sfida di sviluppare un sistema di dialogo in grado di sostenere conversazioni più naturali e precise, affinché l'utente possa davvero “navigare” nelle informazioni. Verranno illustrate sia le soluzioni tecniche che quelle di design, mostrando quali tecnologie e librerie sono state utilizzate, e come sono state integrate per garantire accessibilità e usabilità.

Questa introduzione costituisce la @cap1, dove abbiamo presentato l'idea centrale e il contesto storico e tecnologico in cui si inserisce la ricerca. Nelle sezioni successive sarà passata dapprima in rassegna la letteratura scientifica, con un focus particolare su come l'elaborazione del linguaggio naturale possa fare da ponte tra il mondo visivo e le persone con disabilità. Verranno poi esaminate le principali sfide di carattere tecnico, dalle architetture di intelligenza artificiale fino alle problematiche di accessibilità legate alle interfacce conversazionali.

Nella @nlu-cap entreremo nel merito della *Natural Language Understanding* (NLU). Verrà dapprima analizzato il funzionamento dei chatbot rule-based, con particolare attenzione ad AIML (@aiml-cap). Questa parte permetterà di evidenziare gli aspetti positivi e i limiti di un approccio basato sulla definizione di regole statiche, motivando la scelta di progettare un nuovo sistema di dialogo.\ Successivamente, il discorso passerà alla classificazione degli intenti mediante modelli di tipo neurale (LLM), illustrando come tali modelli possano essere addestrati su dataset etichettati e, se necessario, potenziati tramite tecniche di data augmentation (@classificazione-llm). In ultimo, si discuterà il riconoscimento delle entità (Named Entity Recognition, NER) e la sua importanza per estrarre informazioni chiave dai messaggi degli utenti (@ner-cap).

La @nlg è interamente dedicata alla cosiddetta *Retrieval-Augmented Generation*, un approccio grazie al quale i modelli linguistici possono integrare informazioni provenienti da fonti esterne (basi di conoscenza, corpora o API) e fornire così risposte più aggiornate e contestualizzate. Il capitolo illustra come strutturare il recupero dei dati (Data Retrieval) e come utilizzare le capacità di generazione delle LLM per produrre output più accurati e naturali. Viene inoltre dedicato largo spazio all'analisi della qualità delle risposte e a quali metriche o strategie impiegare per valutarne l'efficacia.

La @engi conclude il lavoro, passando una prospettiva di ingegneria del software: sarà infatti offerta una panoramica dell'architettura complessiva del sistema, spiegando le scelte progettuali e la logica di funzionamento dei componenti chiave. Qui vengono anche esaminate le dipendenze tra i vari moduli, le pipeline di elaborazione e saranno analizzate alcune possibili vie di estenzione o affinamento ulteriore della piattaforma in vista di sviluppi futuri.
